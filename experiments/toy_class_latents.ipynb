{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Experiment: Class Label Latents\n",
    "\n",
    "**Goal**: Test whether loss functions can prevent speckles while encoding information.\n",
    "\n",
    "**Setup**:\n",
    "- 100 learnable 32×32×3 RGB latents (one per class)\n",
    "- Tiny MLP decoder: latent → class prediction\n",
    "- Same losses as real training (InfoNCE, batch InfoNCE, magnitude)\n",
    "\n",
    "**Question**: Can we encode 100 classes smoothly, or do speckles emerge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../phase1')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import loss functions from phase1\n",
    "from infonce_losses import InfoNCEPatchLoss, MagnitudeLoss\n",
    "from batch_infonce_loss import BatchInfoNCELoss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Learnable Latents and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_classes = 100\n",
    "latent_h = 32\n",
    "latent_w = 32\n",
    "latent_c = 3\n",
    "\n",
    "# Learnable latents (one per class)\n",
    "# Initialize with small random values\n",
    "latents = nn.Parameter(torch.randn(num_classes, latent_h, latent_w, latent_c) * 0.1)\n",
    "print(f'Latents shape: {latents.shape}')\n",
    "print(f'Latents parameters: {latents.numel():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny MLP Decoder\n",
    "class TinyClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=32*32*3, hidden_dim=256, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, latents_bhwc):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latents_bhwc: [B, H, W, C] latent images\n",
    "        Returns:\n",
    "            logits: [B, num_classes]\n",
    "        \"\"\"\n",
    "        B = latents_bhwc.shape[0]\n",
    "        x = latents_bhwc.reshape(B, -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "decoder = TinyClassifier().to(device)\n",
    "print(f'\\nDecoder parameters: {sum(p.numel() for p in decoder.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function configuration\n",
    "config = {\n",
    "    'lambda_class': 1.0,        # Classification (must encode class ID)\n",
    "    'lambda_infonce': 2.0,      # Spatial smoothness within latent\n",
    "    'lambda_batch_infonce': 1.0,  # Different classes look different\n",
    "    'lambda_magnitude': 5.0,    # Prevent collapse\n",
    "    \n",
    "    # InfoNCE parameters\n",
    "    'patch_size': 3,\n",
    "    'num_samples': 25,\n",
    "    'temperature': 1.0,\n",
    "    'positive_radius': 3.0,\n",
    "    'negative_radius': 11.0,\n",
    "    \n",
    "    # Batch InfoNCE parameters\n",
    "    'batch_infonce_temperature': 0.5,\n",
    "    'batch_infonce_cross_image_radius': 2.0,\n",
    "    'batch_infonce_num_cross_images': 8,\n",
    "    \n",
    "    # Magnitude\n",
    "    'min_magnitude': 0.3,\n",
    "}\n",
    "\n",
    "# Initialize loss functions\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "infonce_loss_fn = InfoNCEPatchLoss(\n",
    "    patch_size=config['patch_size'],\n",
    "    num_samples=config['num_samples'],\n",
    "    temperature=config['temperature'],\n",
    "    positive_radius=config['positive_radius'],\n",
    "    negative_radius=config['negative_radius']\n",
    ").to(device)\n",
    "\n",
    "batch_infonce_loss_fn = BatchInfoNCELoss(\n",
    "    patch_size=config['patch_size'],\n",
    "    num_samples=config['num_samples'],\n",
    "    temperature=config['batch_infonce_temperature'],\n",
    "    cross_image_radius=config['batch_infonce_cross_image_radius'],\n",
    "    num_cross_images=config['batch_infonce_num_cross_images']\n",
    ").to(device)\n",
    "\n",
    "magnitude_loss_fn = MagnitudeLoss(\n",
    "    min_magnitude=config['min_magnitude']\n",
    ").to(device)\n",
    "\n",
    "print('Loss functions initialized')\n",
    "print(f\"  lambda_class: {config['lambda_class']}\")\n",
    "print(f\"  lambda_infonce: {config['lambda_infonce']}\")\n",
    "print(f\"  lambda_batch_infonce: {config['lambda_batch_infonce']}\")\n",
    "print(f\"  lambda_magnitude: {config['lambda_magnitude']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW([\n",
    "    {'params': [latents], 'lr': 1e-2},\n",
    "    {'params': decoder.parameters(), 'lr': 1e-3}\n",
    "])\n",
    "\n",
    "# Training settings\n",
    "num_steps = 2000\n",
    "batch_size = 32\n",
    "log_interval = 100\n",
    "\n",
    "# Track losses\n",
    "history = {\n",
    "    'total': [],\n",
    "    'class': [],\n",
    "    'infonce': [],\n",
    "    'batch_infonce': [],\n",
    "    'magnitude': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "print(f'Training for {num_steps} steps with batch size {batch_size}...')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "latents_param = latents.to(device)\n",
    "\n",
    "pbar = tqdm(range(num_steps))\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample random batch of class IDs\n",
    "    class_ids = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "    \n",
    "    # Look up corresponding latents\n",
    "    batch_latents = latents_param[class_ids]  # [batch_size, H, W, C]\n",
    "    \n",
    "    # Decode to class predictions\n",
    "    logits = decoder(batch_latents)\n",
    "    \n",
    "    # 1. Classification loss\n",
    "    loss_class = class_loss_fn(logits, class_ids)\n",
    "    \n",
    "    # 2. InfoNCE loss (spatial smoothness within each latent)\n",
    "    loss_infonce = infonce_loss_fn(batch_latents)\n",
    "    \n",
    "    # 3. Batch InfoNCE loss (different classes look different)\n",
    "    if batch_size > 1:\n",
    "        loss_batch_infonce = batch_infonce_loss_fn(batch_latents)\n",
    "    else:\n",
    "        loss_batch_infonce = torch.tensor(0.0, device=device)\n",
    "    \n",
    "    # 4. Magnitude loss\n",
    "    loss_magnitude = magnitude_loss_fn(batch_latents)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = (\n",
    "        config['lambda_class'] * loss_class +\n",
    "        config['lambda_infonce'] * loss_infonce +\n",
    "        config['lambda_batch_infonce'] * loss_batch_infonce +\n",
    "        config['lambda_magnitude'] * loss_magnitude\n",
    "    )\n",
    "    \n",
    "    # Backward\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track metrics\n",
    "    with torch.no_grad():\n",
    "        accuracy = (logits.argmax(dim=1) == class_ids).float().mean()\n",
    "    \n",
    "    history['total'].append(total_loss.item())\n",
    "    history['class'].append(loss_class.item())\n",
    "    history['infonce'].append(loss_infonce.item())\n",
    "    history['batch_infonce'].append(loss_batch_infonce.item())\n",
    "    history['magnitude'].append(loss_magnitude.item())\n",
    "    history['accuracy'].append(accuracy.item())\n",
    "    \n",
    "    # Update progress bar\n",
    "    if step % log_interval == 0:\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{total_loss.item():.3f}\",\n",
    "            'acc': f\"{accuracy.item():.3f}\",\n",
    "            'class': f\"{loss_class.item():.3f}\",\n",
    "            'info': f\"{loss_infonce.item():.3f}\",\n",
    "            'batch': f\"{loss_batch_infonce.item():.3f}\",\n",
    "            'mag': f\"{loss_magnitude.item():.3f}\"\n",
    "        })\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "axes[0, 0].plot(history['total'])\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(history['accuracy'])\n",
    "axes[0, 1].set_title('Classification Accuracy')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylim([0, 1.05])\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[0, 2].plot(history['class'])\n",
    "axes[0, 2].set_title('Classification Loss')\n",
    "axes[0, 2].set_xlabel('Step')\n",
    "axes[0, 2].grid(True)\n",
    "\n",
    "axes[1, 0].plot(history['infonce'])\n",
    "axes[1, 0].set_title('InfoNCE Loss (smoothness)')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(history['batch_infonce'])\n",
    "axes[1, 1].set_title('Batch InfoNCE Loss (diversity)')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "axes[1, 2].plot(history['magnitude'])\n",
    "axes[1, 2].set_title('Magnitude Loss')\n",
    "axes[1, 2].set_xlabel('Step')\n",
    "axes[1, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Final accuracy: {history[\"accuracy\"][-1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization: Learned Latents\n",
    "\n",
    "Show all 100 learned latent images. Look for speckles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_to_rgb(latent_tensor):\n",
    "    \"\"\"Convert [H, W, 3] tensor to displayable RGB, normalized to [0, 1]\"\"\"\n",
    "    rgb = latent_tensor.detach().cpu().numpy()\n",
    "    # Normalize to [0, 1] for display\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)\n",
    "    return np.clip(rgb, 0, 1)\n",
    "\n",
    "# Show grid of all latents\n",
    "num_rows = 10\n",
    "num_cols = 10\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    \n",
    "    rgb = latent_to_rgb(latents_param[i])\n",
    "    \n",
    "    axes[row, col].imshow(rgb)\n",
    "    axes[row, col].set_title(f'{i}', fontsize=8)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Learned Latents for 100 Classes', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nExamine the images above:')\n",
    "print('  - Are they smooth or speckled?')\n",
    "print('  - Do different classes look different?')\n",
    "print('  - Is there visible structure?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis: Speckle Detection\n",
    "\n",
    "Quantify high-frequency content (speckles) in the latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_high_freq_energy(latent):\n",
    "    \"\"\"Measure high-frequency energy using gradient magnitude\"\"\"\n",
    "    # Compute gradients (high freq = large gradients)\n",
    "    grad_y = latent[1:, :, :] - latent[:-1, :, :]\n",
    "    grad_x = latent[:, 1:, :] - latent[:, :-1, :]\n",
    "    \n",
    "    # Average gradient magnitude\n",
    "    return (grad_y.abs().mean() + grad_x.abs().mean()) / 2\n",
    "\n",
    "# Compute for all latents\n",
    "with torch.no_grad():\n",
    "    high_freq_energies = []\n",
    "    for i in range(num_classes):\n",
    "        energy = compute_high_freq_energy(latents_param[i])\n",
    "        high_freq_energies.append(energy.item())\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(high_freq_energies, bins=30)\n",
    "plt.xlabel('High-Frequency Energy (gradient magnitude)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of High-Frequency Content Across Latents')\n",
    "plt.axvline(np.mean(high_freq_energies), color='r', linestyle='--', label=f'Mean: {np.mean(high_freq_energies):.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Average high-frequency energy: {np.mean(high_freq_energies):.4f}')\n",
    "print(f'Std dev: {np.std(high_freq_energies):.4f}')\n",
    "print(f'\\nHigher values = more speckles/high-frequency content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment: Test Different Loss Configurations\n",
    "\n",
    "Compare results with different settings to see what prevents speckles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current results\n",
    "baseline_latents = latents_param.detach().clone()\n",
    "baseline_energy = np.mean(high_freq_energies)\n",
    "baseline_accuracy = history['accuracy'][-1]\n",
    "\n",
    "print('Baseline (current) results saved:')\n",
    "print(f'  High-freq energy: {baseline_energy:.4f}')\n",
    "print(f'  Final accuracy: {baseline_accuracy:.3f}')\n",
    "print(f'  Config: lambda_infonce={config[\"lambda_infonce\"]}, positive_radius={config[\"positive_radius\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Higher InfoNCE Weight\n",
    "\n",
    "Test if increasing lambda_infonce reduces speckles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can copy the training loop above and modify config['lambda_infonce']\n",
    "# Then compare the resulting latents visually and quantitatively\n",
    "\n",
    "print('To experiment:')\n",
    "print('1. Modify config[\"lambda_infonce\"] (try 5.0, 10.0)')\n",
    "print('2. Re-run training loop')\n",
    "print('3. Compare learned latents')\n",
    "print('4. Compare high-frequency energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Future: Test L2 Distance vs Cosine Similarity\n",
    "\n",
    "Modify InfoNCEPatchLoss to use L2 distance instead of cosine similarity.\n",
    "This would require editing the loss function to avoid normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Next step: Implement L2-based InfoNCE variant')\n",
    "print('Expected: L2 distance constrains both direction AND magnitude')\n",
    "print('Hypothesis: This should prevent magnitude-based speckles')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
