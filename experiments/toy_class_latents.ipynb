{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jtooates/blind_lm/blob/main/experiments/toy_class_latents.ipynb)\n",
    "\n",
    "# Toy Experiment: Class Label Latents\n",
    "\n",
    "**Goal**: Test whether loss functions can prevent speckles while encoding information.\n",
    "\n",
    "**Setup**:\n",
    "- 100 learnable 32×32×3 RGB latents (one per class)\n",
    "- Tiny MLP decoder: latent → class prediction\n",
    "- Same losses as real training (InfoNCE, batch InfoNCE, magnitude)\n",
    "\n",
    "**Question**: Can we encode 100 classes smoothly, or do speckles emerge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup: Clone or update repository\n",
    "import os\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Running on Google Colab\")\n",
    "    \n",
    "    repo_dir = '/content/blind_lm'\n",
    "    repo_url = 'https://github.com/jtooates/blind_lm.git'\n",
    "    \n",
    "    if os.path.exists(repo_dir):\n",
    "        print(\"Repository exists. Pulling latest changes...\")\n",
    "        %cd {repo_dir}\n",
    "        !git pull origin main\n",
    "        print(\"✓ Repository updated\")\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        %cd /content\n",
    "        !git clone {repo_url}\n",
    "        print(\"✓ Repository cloned\")\n",
    "    \n",
    "    # Navigate to experiments directory\n",
    "    %cd {repo_dir}/experiments\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    # Assume we're already in the right directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add phase1 to path\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # On Colab, we're in /content/blind_lm/experiments\n",
    "    sys.path.insert(0, '/content/blind_lm/phase1')\n",
    "else:\n",
    "    # Running locally\n",
    "    sys.path.insert(0, '../phase1')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import loss functions from phase1\n",
    "from infonce_losses import InfoNCEPatchLoss, InfoNCEPatchLossL2, MagnitudeLoss\n",
    "from batch_infonce_loss import BatchInfoNCELoss\n",
    "\n",
    "# Mumford-Shah loss for bounded homogeneous regions\n",
    "class MumfordShahLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Mumford-Shah loss for piecewise constant regions.\n",
    "    \n",
    "    Encourages the latent to be smooth within regions but allows\n",
    "    sharp boundaries between regions.\n",
    "    \n",
    "    The key insight: L2 smoothness term heavily penalizes gradients\n",
    "    (wants constant regions), but L1 boundary term allows sparse\n",
    "    boundaries to exist (linear cost doesn't explode for edges).\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, beta=0.1, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Smoothness within regions (L2)\n",
    "        self.beta = beta    # Boundary sparsity (L1)\n",
    "        self.epsilon = epsilon  # Numerical stability\n",
    "    \n",
    "    def forward(self, latent):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent: [B, H, W, C] RGB latent\n",
    "        Returns:\n",
    "            Scalar loss\n",
    "        \"\"\"\n",
    "        # Compute gradients in horizontal and vertical directions\n",
    "        diff_h = latent[:, 1:, :, :] - latent[:, :-1, :, :]  # [B, H-1, W, C]\n",
    "        diff_w = latent[:, :, 1:, :] - latent[:, :, :-1, :]  # [B, H, W-1, C]\n",
    "        \n",
    "        # Smoothness term: L2 penalty on gradients\n",
    "        # Heavily penalizes gradients - wants regions to be constant\n",
    "        smoothness = (diff_h.pow(2).mean() + diff_w.pow(2).mean())\n",
    "        \n",
    "        # Boundary term: L1 penalty on gradients\n",
    "        # Allows sparse boundaries to exist\n",
    "        # Add epsilon for numerical stability of sqrt\n",
    "        boundary = (\n",
    "            (diff_h.pow(2).sum(dim=-1, keepdim=True) + self.epsilon).sqrt().mean() +\n",
    "            (diff_w.pow(2).sum(dim=-1, keepdim=True) + self.epsilon).sqrt().mean()\n",
    "        )\n",
    "        \n",
    "        return self.alpha * smoothness + self.beta * boundary\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Learnable Latents and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: torch.Size([100, 32, 32, 3])\n",
      "Latents parameters: 307,200\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "num_classes = 100\n",
    "latent_h = 32\n",
    "latent_w = 32\n",
    "latent_c = 3\n",
    "\n",
    "# Learnable latents (one per class)\n",
    "# Initialize with small random values\n",
    "latents = nn.Parameter(torch.randn(num_classes, latent_h, latent_w, latent_c) * 0.1)\n",
    "print(f'Latents shape: {latents.shape}')\n",
    "print(f'Latents parameters: {latents.numel():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoder parameters: 812,388\n"
     ]
    }
   ],
   "source": [
    "# Tiny MLP Decoder\n",
    "class TinyClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=32*32*3, hidden_dim=256, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, latents_bhwc):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latents_bhwc: [B, H, W, C] latent images\n",
    "        Returns:\n",
    "            logits: [B, num_classes]\n",
    "        \"\"\"\n",
    "        B = latents_bhwc.shape[0]\n",
    "        x = latents_bhwc.reshape(B, -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "decoder = TinyClassifier().to(device)\n",
    "print(f'\\nDecoder parameters: {sum(p.numel() for p in decoder.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using InfoNCE with cosine similarity (direction only)\n",
      "Loss functions initialized\n",
      "  lambda_class: 1.0\n",
      "  lambda_infonce: 0.0\n",
      "  lambda_batch_infonce: 0.0\n",
      "  lambda_magnitude: 10.0\n",
      "  lambda_mumford_shah: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Loss function configuration\n",
    "config = {\n",
    "    'use_l2_infonce': False,     # NEW: Set to True to use L2 distance instead of cosine similarity\n",
    "    \n",
    "    'lambda_class': 1.0,        # Classification (must encode class ID)\n",
    "    'lambda_infonce': 0.0,      # Spatial smoothness within latent\n",
    "    'lambda_batch_infonce': 0.0,  # Different classes look different\n",
    "    'lambda_magnitude': 10.0,    # Prevent collapse\n",
    "    'lambda_mumford_shah': 2.0, # NEW: Piecewise constant regions with boundaries\n",
    "    \n",
    "    # InfoNCE parameters\n",
    "    'patch_size': 5,\n",
    "    'num_samples': 25,\n",
    "    'temperature': 1.0,\n",
    "    'positive_radius': 7.0,\n",
    "    'negative_radius': 14.0,\n",
    "    \n",
    "    # Batch InfoNCE parameters\n",
    "    'batch_infonce_temperature': 1.0,\n",
    "    'batch_infonce_cross_image_radius': 5.0,\n",
    "    'batch_infonce_num_cross_images': 8,\n",
    "    \n",
    "    # Magnitude\n",
    "    'min_magnitude': 0.3,\n",
    "    \n",
    "    # Mumford-Shah parameters\n",
    "    'mumford_shah_alpha': 10.0,  # NEW: Within-region smoothness (L2)\n",
    "    'mumford_shah_beta': 2.0,    # NEW: Boundary sparsity (L1)\n",
    "}\n",
    "\n",
    "# Initialize loss functions\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Choose InfoNCE variant\n",
    "if config['use_l2_infonce']:\n",
    "    print('Using InfoNCE with L2 distance (constrains magnitude + direction)')\n",
    "    infonce_loss_fn = InfoNCEPatchLossL2(\n",
    "        patch_size=config['patch_size'],\n",
    "        num_samples=config['num_samples'],\n",
    "        temperature=config['temperature'],\n",
    "        positive_radius=config['positive_radius'],\n",
    "        negative_radius=config['negative_radius']\n",
    "    ).to(device)\n",
    "else:\n",
    "    print('Using InfoNCE with cosine similarity (direction only)')\n",
    "    infonce_loss_fn = InfoNCEPatchLoss(\n",
    "        patch_size=config['patch_size'],\n",
    "        num_samples=config['num_samples'],\n",
    "        temperature=config['temperature'],\n",
    "        positive_radius=config['positive_radius'],\n",
    "        negative_radius=config['negative_radius']\n",
    "    ).to(device)\n",
    "\n",
    "batch_infonce_loss_fn = BatchInfoNCELoss(\n",
    "    patch_size=config['patch_size'],\n",
    "    num_samples=config['num_samples'],\n",
    "    temperature=config['batch_infonce_temperature'],\n",
    "    cross_image_radius=config['batch_infonce_cross_image_radius'],\n",
    "    num_cross_images=config['batch_infonce_num_cross_images']\n",
    ").to(device)\n",
    "\n",
    "magnitude_loss_fn = MagnitudeLoss(\n",
    "    min_magnitude=config['min_magnitude']\n",
    ").to(device)\n",
    "\n",
    "mumford_shah_loss_fn = MumfordShahLoss(\n",
    "    alpha=config['mumford_shah_alpha'],\n",
    "    beta=config['mumford_shah_beta']\n",
    ").to(device)\n",
    "\n",
    "print('Loss functions initialized')\n",
    "print(f\"  lambda_class: {config['lambda_class']}\")\n",
    "print(f\"  lambda_infonce: {config['lambda_infonce']}\")\n",
    "print(f\"  lambda_batch_infonce: {config['lambda_batch_infonce']}\")\n",
    "print(f\"  lambda_magnitude: {config['lambda_magnitude']}\")\n",
    "print(f\"  lambda_mumford_shah: {config['lambda_mumford_shah']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500 steps with batch size 32...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW([\n",
    "    {'params': [latents], 'lr': 1e-2},\n",
    "    {'params': decoder.parameters(), 'lr': 1e-3}\n",
    "])\n",
    "\n",
    "# Training settings\n",
    "num_steps = 500\n",
    "batch_size = 32\n",
    "log_interval = 10\n",
    "\n",
    "# Track losses\n",
    "history = {\n",
    "    'total': [],\n",
    "    'class': [],\n",
    "    'infonce': [],\n",
    "    'batch_infonce': [],\n",
    "    'magnitude': [],\n",
    "    'mumford_shah': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "print(f'Training for {num_steps} steps with batch size {batch_size}...')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop\nlatents_param = latents.to(device)\n\npbar = tqdm(range(num_steps), ncols = 150)\nfor step in pbar:\n    optimizer.zero_grad()\n    \n    # Sample random batch of class IDs\n    class_ids = torch.randint(0, num_classes, (batch_size,), device=device)\n    \n    # Look up corresponding latents\n    batch_latents = latents_param[class_ids]  # [batch_size, H, W, C]\n    \n    # Decode to class predictions\n    logits = decoder(batch_latents)\n    \n    # 1. Classification loss\n    if config['lambda_class'] == 0:\n        loss_class = torch.tensor(0.0, device=device)\n    else:\n        loss_class = class_loss_fn(logits, class_ids)\n    \n    # 2. InfoNCE loss (spatial smoothness within each latent)\n    if config['lambda_infonce'] == 0:\n        loss_infonce = torch.tensor(0.0, device=device)\n    else:\n        loss_infonce = infonce_loss_fn(batch_latents)\n    \n    # 3. Batch InfoNCE loss (different classes look different)\n    if config['lambda_batch_infonce'] == 0:\n        loss_batch_infonce = torch.tensor(0.0, device=device)\n    else:\n        if batch_size > 1:\n            loss_batch_infonce = batch_infonce_loss_fn(batch_latents)\n        else:\n            loss_batch_infonce = torch.tensor(0.0, device=device)\n    \n    # 4. Magnitude loss\n    if config['lambda_magnitude'] == 0:\n        loss_magnitude = torch.tensor(0.0, device=device)\n    else:\n        loss_magnitude = magnitude_loss_fn(batch_latents)\n    \n    # 5. Mumford-Shah loss (piecewise constant regions)\n    if config['lambda_mumford_shah'] == 0:\n        loss_mumford_shah = torch.tensor(0.0, device=device)\n    else:\n        loss_mumford_shah = mumford_shah_loss_fn(batch_latents)\n    \n    # Total loss\n    total_loss = (\n        config['lambda_class'] * loss_class +\n        config['lambda_infonce'] * loss_infonce +\n        config['lambda_batch_infonce'] * loss_batch_infonce +\n        config['lambda_magnitude'] * loss_magnitude +\n        config['lambda_mumford_shah'] * loss_mumford_shah\n    )\n    \n    # Backward\n    total_loss.backward()\n    optimizer.step()\n    \n    # Track metrics\n    with torch.no_grad():\n        accuracy = (logits.argmax(dim=1) == class_ids).float().mean()\n    \n    history['total'].append(total_loss.item())\n    history['class'].append(loss_class.item())\n    history['infonce'].append(loss_infonce.item())\n    history['batch_infonce'].append(loss_batch_infonce.item())\n    history['magnitude'].append(loss_magnitude.item())\n    history['mumford_shah'].append(loss_mumford_shah.item())\n    history['accuracy'].append(accuracy.item())\n    \n    # Update progress bar\n    if step % log_interval == 0:\n        pbar.set_postfix({\n            'loss': f\"{total_loss.item():.2f}\",\n            'acc': f\"{accuracy.item():.2f}\",\n            'cls': f\"{loss_class.item():.2f}\",\n            'inf': f\"{loss_infonce.item():.2f}\",\n            'bat': f\"{loss_batch_infonce.item():.2f}\",\n            'mag': f\"{loss_magnitude.item():.2f}\",\n            'ms': f\"{loss_mumford_shah.item():.2f}\"\n        })\n\nprint('\\nTraining complete!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "axes[0, 0].plot(history['total'])\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(history['accuracy'])\n",
    "axes[0, 1].set_title('Classification Accuracy')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylim([0, 1.05])\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[0, 2].plot(history['class'])\n",
    "axes[0, 2].set_title('Classification Loss')\n",
    "axes[0, 2].set_xlabel('Step')\n",
    "axes[0, 2].grid(True)\n",
    "\n",
    "axes[1, 0].plot(history['infonce'])\n",
    "axes[1, 0].set_title('InfoNCE Loss (smoothness)')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(history['batch_infonce'], label='Batch InfoNCE')\n",
    "axes[1, 1].plot(history['mumford_shah'], label='Mumford-Shah')\n",
    "axes[1, 1].set_title('Batch InfoNCE + Mumford-Shah')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "axes[1, 2].plot(history['magnitude'])\n",
    "axes[1, 2].set_title('Magnitude Loss')\n",
    "axes[1, 2].set_xlabel('Step')\n",
    "axes[1, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Final accuracy: {history[\"accuracy\"][-1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization: Learned Latents\n",
    "\n",
    "Show all 100 learned latent images. Look for speckles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_to_rgb(latent_tensor):\n",
    "    \"\"\"Convert [H, W, 3] tensor to displayable RGB, normalized to [0, 1]\"\"\"\n",
    "    rgb = latent_tensor.detach().cpu().numpy()\n",
    "    # Normalize to [0, 1] for display\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)\n",
    "    return np.clip(rgb, 0, 1)\n",
    "\n",
    "# Show grid of all latents\n",
    "num_rows = 10\n",
    "num_cols = 10\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    \n",
    "    rgb = latent_to_rgb(latents_param[i])\n",
    "    \n",
    "    axes[row, col].imshow(rgb)\n",
    "    axes[row, col].set_title(f'{i}', fontsize=8)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Learned Latents for 100 Classes', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nExamine the images above:')\n",
    "print('  - Are they smooth or speckled?')\n",
    "print('  - Do different classes look different?')\n",
    "print('  - Is there visible structure?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis: Speckle Detection\n",
    "\n",
    "Quantify high-frequency content (speckles) in the latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_high_freq_energy(latent):\n",
    "    \"\"\"Measure high-frequency energy using gradient magnitude\"\"\"\n",
    "    # Compute gradients (high freq = large gradients)\n",
    "    grad_y = latent[1:, :, :] - latent[:-1, :, :]\n",
    "    grad_x = latent[:, 1:, :] - latent[:, :-1, :]\n",
    "    \n",
    "    # Average gradient magnitude\n",
    "    return (grad_y.abs().mean() + grad_x.abs().mean()) / 2\n",
    "\n",
    "# Compute for all latents\n",
    "with torch.no_grad():\n",
    "    high_freq_energies = []\n",
    "    for i in range(num_classes):\n",
    "        energy = compute_high_freq_energy(latents_param[i])\n",
    "        high_freq_energies.append(energy.item())\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(high_freq_energies, bins=30)\n",
    "plt.xlabel('High-Frequency Energy (gradient magnitude)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of High-Frequency Content Across Latents')\n",
    "plt.axvline(np.mean(high_freq_energies), color='r', linestyle='--', label=f'Mean: {np.mean(high_freq_energies):.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Average high-frequency energy: {np.mean(high_freq_energies):.4f}')\n",
    "print(f'Std dev: {np.std(high_freq_energies):.4f}')\n",
    "print(f'\\nHigher values = more speckles/high-frequency content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment: Test Different Loss Configurations\n",
    "\n",
    "Compare results with different settings to see what prevents speckles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current results\n",
    "baseline_latents = latents_param.detach().clone()\n",
    "baseline_energy = np.mean(high_freq_energies)\n",
    "baseline_accuracy = history['accuracy'][-1]\n",
    "\n",
    "print('Baseline (current) results saved:')\n",
    "print(f'  High-freq energy: {baseline_energy:.4f}')\n",
    "print(f'  Final accuracy: {baseline_accuracy:.3f}')\n",
    "print(f'  Config: lambda_infonce={config[\"lambda_infonce\"]}, positive_radius={config[\"positive_radius\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Higher InfoNCE Weight\n",
    "\n",
    "Test if increasing lambda_infonce reduces speckles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can copy the training loop above and modify config['lambda_infonce']\n",
    "# Then compare the resulting latents visually and quantitatively\n",
    "\n",
    "print('To experiment:')\n",
    "print('1. Modify config[\"lambda_infonce\"] (try 5.0, 10.0)')\n",
    "print('2. Re-run training loop')\n",
    "print('3. Compare learned latents')\n",
    "print('4. Compare high-frequency energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment: L2 Distance vs Cosine Similarity\n",
    "\n",
    "**Hypothesis**: L2 distance constrains both direction AND magnitude, preventing checkerboard speckles.\n",
    "\n",
    "**To test**:\n",
    "1. Set `config['use_l2_infonce'] = True` in cell 6\n",
    "2. Re-run cells 6-15\n",
    "3. Compare learned latents and high-frequency energy\n",
    "\n",
    "**Expected result**: L2 variant should produce smoother latents with less checkerboard texture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('L2-based InfoNCE variant is now available!')\n",
    "print()\n",
    "print('To compare:')\n",
    "print('  1. Cosine (current): use_l2_infonce=False')\n",
    "print('  2. L2 distance: use_l2_infonce=True')\n",
    "print()\n",
    "print('Expected difference:')\n",
    "print('  - Cosine: May produce checkerboards (magnitude variations allowed)')\n",
    "print('  - L2: Should be smoother (magnitude constrained)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment: Mumford-Shah Loss for Bounded Regions\n",
    "\n",
    "**What it does**: Creates piecewise constant regions with sharp boundaries (object-like shapes).\n",
    "\n",
    "**How it works**:\n",
    "- **L2 smoothness term** (alpha): Heavily penalizes gradients → wants constant regions\n",
    "- **L1 boundary term** (beta): Lightly penalizes gradients → allows sparse boundaries\n",
    "\n",
    "**To enable**:\n",
    "1. Set `config['lambda_mumford_shah'] = 10.0` (or higher) in cell 7\n",
    "2. Adjust `alpha` (within-region smoothness, default 10.0) and `beta` (boundary sparsity, default 0.5)\n",
    "3. Re-run training (cells 7-15)\n",
    "\n",
    "**Parameter tuning**:\n",
    "- **alpha too high** → everything becomes one color (increase beta or decrease alpha)\n",
    "- **alpha too low** → speckles remain (increase alpha)\n",
    "- **beta too low** → over-segmented into tiny regions (increase beta)\n",
    "- **beta too high** → can't form distinct regions (decrease beta)\n",
    "- Start with `alpha/beta ≈ 20` ratio\n",
    "\n",
    "**Expected results**:\n",
    "- Large smooth color regions within each latent\n",
    "- Sharp boundaries between regions (blob/shape patterns)\n",
    "- Fewer high-frequency speckles\n",
    "- Geometric structure emerges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}