{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Word Attribution Analysis\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jtooates/blind_lm/blob/main/word_attribution_analysis.ipynb)\n\nThis notebook analyzes which parts of the RGB latent are responsible for generating each word.\n\n**Method**: Gradient-based attribution - compute ∂(logit)/∂(latent) to see which pixels influence each word's prediction."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Environment Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect environment (Colab vs Local)\ntry:\n    from google.colab import drive\n    IN_COLAB = True\n    print(\"✓ Running in Google Colab\")\nexcept:\n    IN_COLAB = False\n    print(\"✓ Running locally\")"
  },
  {
   "cell_type": "code",
   "source": "# Install dependencies (if in Colab)\nif IN_COLAB:\n    print(\"Installing dependencies...\")\n    !pip install -q transformers torch matplotlib numpy\n    print(\"✓ Dependencies installed\")\nelse:\n    print(\"✓ Skipping install (assuming dependencies already available locally)\")\n\n# Suppress tokenizer warning\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install dependencies\nprint(\"Installing dependencies...\")\n!pip install -q transformers torch matplotlib numpy\n\n# Suppress tokenizer warning\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nprint(\"✓ Dependencies installed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Mount Google Drive (for Colab)\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    IN_COLAB = True\n    print(\"✓ Google Drive mounted\")\n    print(\"✓ Checkpoints location: /content/drive/MyDrive/blind_lm_outputs/\")\nexcept:\n    IN_COLAB = False\n    print(\"✓ Running locally\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## 2. Load Model",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport json\nimport sys\nimport os\n\n# Add phase1 to path (handle both Colab and local)\nif IN_COLAB:\n    # In Colab, we're already in /content/blind_lm after cloning\n    sys.path.insert(0, '/content/blind_lm/phase1')\n    os.chdir('/content/blind_lm')  # Ensure we're in repo root\nelse:\n    # Local: assume we're running from repo root\n    sys.path.insert(0, 'phase1')\n\nfrom model import create_model\nfrom decoder_nonar import create_decoder\nfrom transformers import AutoTokenizer\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Determine checkpoint location based on environment\nif IN_COLAB:\n    checkpoint_dir = Path('/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce')\nelse:\n    checkpoint_dir = Path('outputs/phase1_rgb_infonce')\n\ncheckpoint_path = checkpoint_dir / 'checkpoint_latest.pt'\nconfig_path = checkpoint_dir / 'config.json'\n\n# Check if files exist\nif not checkpoint_path.exists():\n    print(f\"❌ Checkpoint not found at {checkpoint_path}\")\n    print(\"\\nPlease ensure you have a trained model.\")\n    if IN_COLAB:\n        print(\"Expected location: /content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/\")\n        print(\"You can train a model using phase1_colab_training.ipynb\")\n    else:\n        print(\"Expected location: outputs/phase1_rgb_infonce/\")\n    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n\nif not config_path.exists():\n    print(f\"❌ Config not found at {config_path}\")\n    raise FileNotFoundError(f\"Config not found: {config_path}\")\n\nprint(f\"✓ Found checkpoint: {checkpoint_path}\")\nprint(f\"✓ Found config: {config_path}\")\n\n# Load config\nwith open(config_path) as f:\n    config = json.load(f)\n\nprint(f\"\\nModel configuration:\")\nprint(f\"  Channels: {config['model']['num_channels']} (RGB)\")\nprint(f\"  Grid size: {config['model']['grid_size']}x{config['model']['grid_size']}\")\nprint(f\"  Hidden size: {config['model']['hidden_size']}\")\n\n# Create models\nencoder = create_model(config['model']).to(device)\ndecoder = create_decoder(config['decoder']).to(device)\n\n# Load checkpoint\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nencoder.load_state_dict(checkpoint['encoder_state_dict'])\ndecoder.load_state_dict(checkpoint['decoder_state_dict'])\nencoder.eval()\ndecoder.eval()\n\nprint(f\"\\n✓ Loaded checkpoint from step {checkpoint['step']}\")\n\n# Create tokenizer\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"✓ Models loaded and ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## 3. Attribution Functions",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def compute_word_attribution(latent, decoder, word_idx, predicted_token_id):\n    \"\"\"\n    Compute gradient-based attribution for a specific word.\n    \n    Args:\n        latent: [1, H, W, C] latent tensor (requires grad)\n        decoder: decoder model\n        word_idx: position of word in sequence\n        predicted_token_id: token ID that was predicted\n    \n    Returns:\n        heatmap: [H, W] numpy array of attribution scores\n    \"\"\"\n    # Forward pass\n    logits = decoder(latent)  # [1, seq_len, vocab_size]\n    \n    # Get logit for predicted token at this position\n    target_logit = logits[0, word_idx, predicted_token_id]\n    \n    # Backward pass\n    target_logit.backward(retain_graph=True)\n    \n    # Get gradient magnitude\n    grad = latent.grad.abs()  # [1, H, W, C]\n    \n    # Average across RGB channels to get [H, W] heatmap\n    heatmap = grad[0].mean(dim=-1)  # [H, W]\n    \n    return heatmap.detach().cpu().numpy()\n\n\ndef latent_to_rgb(latent_tensor):\n    \"\"\"\n    Convert latent [H, W, C] to displayable RGB [H, W, 3].\n    \"\"\"\n    rgb = latent_tensor.cpu().numpy()\n    # Normalize from [-1.5, 1.5] to [0, 1]\n    rgb = (rgb + 1.5) / 3.0\n    rgb = np.clip(rgb, 0, 1)\n    return rgb\n\n\ndef visualize_word_attributions(encoder, decoder, tokenizer, sentence, device):\n    \"\"\"\n    Generate visualization showing which latent pixels are important for each word.\n    \n    Display:\n    - Row 0: Original RGB latent (shown once)\n    - Row 1: Heatmap for each word\n    - Row 2: RGB + heatmap overlay for each word\n    \"\"\"\n    # Tokenize input\n    inputs = tokenizer(sentence, return_tensors='pt', padding='max_length', \n                      truncation=True, max_length=64)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    \n    # Encode (no grad needed here)\n    with torch.no_grad():\n        latent_base = encoder(input_ids, attention_mask)  # [1, 32, 32, 3]\n        \n        # Decode to get predictions\n        logits = decoder(latent_base)\n        predicted_ids = logits.argmax(dim=-1)\n    \n    # Get predicted tokens (skip padding)\n    num_tokens = attention_mask[0].sum().item()\n    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_ids[0][:num_tokens])\n    predicted_token_ids = predicted_ids[0][:num_tokens].cpu().tolist()\n    \n    # Convert to displayable RGB\n    rgb = latent_to_rgb(latent_base[0])\n    \n    # Compute heatmaps for each word\n    print(f\"Computing attributions for {len(predicted_tokens)} tokens...\")\n    heatmaps = []\n    \n    for i, (token, token_id) in enumerate(zip(predicted_tokens, predicted_token_ids)):\n        # Create latent that requires grad for this computation\n        latent = latent_base.clone().detach().requires_grad_(True)\n        \n        # Compute attribution\n        heatmap = compute_word_attribution(latent, decoder, i, token_id)\n        heatmaps.append(heatmap)\n        \n        # Clear gradients\n        if latent.grad is not None:\n            latent.grad.zero_()\n    \n    print(\"✓ Attributions computed\")\n    \n    # Create visualization\n    n_tokens = len(predicted_tokens)\n    fig = plt.figure(figsize=(n_tokens * 2, 8))\n    \n    # Row 0: Original RGB latent (spanning all columns)\n    ax_rgb = plt.subplot(3, 1, 1)\n    ax_rgb.imshow(rgb)\n    ax_rgb.set_title(f'RGB Latent for: \"{sentence}\"', fontsize=12, fontweight='bold')\n    ax_rgb.axis('off')\n    \n    # Rows 1-2: Heatmaps and overlays for each word\n    for i, (token, heatmap) in enumerate(zip(predicted_tokens, heatmaps)):\n        # Row 1: Heatmap\n        ax_heat = plt.subplot(3, n_tokens, n_tokens + i + 1)\n        im = ax_heat.imshow(heatmap, cmap='hot', interpolation='nearest')\n        ax_heat.set_title(f'{token}', fontsize=10)\n        ax_heat.axis('off')\n        \n        # Row 2: Overlay\n        ax_overlay = plt.subplot(3, n_tokens, 2 * n_tokens + i + 1)\n        ax_overlay.imshow(rgb)\n        ax_overlay.imshow(heatmap, cmap='hot', alpha=0.5, interpolation='nearest')\n        ax_overlay.axis('off')\n    \n    plt.tight_layout()\n    \n    # Print reconstruction\n    reconstruction = tokenizer.decode(predicted_ids[0][:num_tokens], skip_special_tokens=True)\n    print(f\"\\nOriginal:       {sentence}\")\n    print(f\"Reconstruction: {reconstruction}\")\n    if sentence.strip() == reconstruction.strip():\n        print(\"✓ EXACT MATCH\")\n    \n    return fig, predicted_tokens, heatmaps\n\nprint(\"✓ Attribution functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Interactive Analysis\n\nEnter a sentence to analyze which parts of the latent are responsible for each word."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Analysis Tips\n\n**What to look for:**\n\n1. **Color words** (red, blue, yellow): Do they activate specific colored regions?\n2. **Spatial words** (under, right, left): Do they activate specific spatial patterns?\n3. **Object words** (cube, block, box): Do they have consistent activation patterns?\n4. **Function words** (the, is): Typically should have low/diffuse activation\n\n**Heatmap interpretation:**\n- **Bright (yellow/white)**: High importance - this pixel strongly influenced the word\n- **Dark (red/black)**: Low importance - this pixel didn't affect the word much\n\n**Questions to explore:**\n- Do different occurrences of \"the\" activate different regions?\n- Do color words consistently activate the same colored blobs?\n- Do spatial relations show positional patterns in the latent?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Try More Sentences\n\nRun the cell below multiple times with different sentences to explore patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for sentence\n",
    "sentence = input(\"Enter a sentence to analyze: \")\n",
    "\n",
    "if not sentence.strip():\n",
    "    sentence = \"the red cube is under the yellow block\"  # Default example\n",
    "    print(f\"Using default: {sentence}\")\n",
    "\n",
    "# Generate visualization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "fig, tokens, heatmaps = visualize_word_attributions(encoder, decoder, tokenizer, sentence, device)\n",
    "print(\"=\"*70)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Tips\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "1. **Color words** (red, blue, yellow): Do they activate specific colored regions?\n",
    "2. **Spatial words** (under, right, left): Do they activate specific spatial patterns?\n",
    "3. **Object words** (cube, block, box): Do they have consistent activation patterns?\n",
    "4. **Function words** (the, is): Typically should have low/diffuse activation\n",
    "\n",
    "**Heatmap interpretation:**\n",
    "- **Bright (yellow/white)**: High importance - this pixel strongly influenced the word\n",
    "- **Dark (red/black)**: Low importance - this pixel didn't affect the word much\n",
    "\n",
    "**Questions to explore:**\n",
    "- Do different occurrences of \"the\" activate different regions?\n",
    "- Do color words consistently activate the same colored blobs?\n",
    "- Do spatial relations show positional patterns in the latent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try More Sentences\n",
    "\n",
    "Run the cell below multiple times with different sentences to explore patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences to try:\n",
    "examples = [\n",
    "    \"the red cube is under the yellow block\",\n",
    "    \"the blue box is right of the green sphere\",\n",
    "    \"the yellow block is on the red cube\",\n",
    "    \"the green cube is left of the blue box\",\n",
    "]\n",
    "\n",
    "print(\"Example sentences you can try:\")\n",
    "for i, ex in enumerate(examples, 1):\n",
    "    print(f\"{i}. {ex}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "sentence = input(\"Enter a sentence (or leave blank for random example): \")\n",
    "\n",
    "if not sentence.strip():\n",
    "    import random\n",
    "    sentence = random.choice(examples)\n",
    "    print(f\"Using random example: {sentence}\")\n",
    "\n",
    "fig, tokens, heatmaps = visualize_word_attributions(encoder, decoder, tokenizer, sentence, device)\n",
    "print(\"=\"*70)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}