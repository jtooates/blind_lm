{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Attribution Analysis\n",
    "\n",
    "This notebook analyzes which parts of the RGB latent are responsible for generating each word.\n",
    "\n",
    "**Method**: Gradient-based attribution - compute ∂(logit)/∂(latent) to see which pixels influence each word's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Add phase1 to path\n",
    "sys.path.append('phase1')\n",
    "\n",
    "from model import create_model\n",
    "from decoder_nonar import create_decoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard checkpoint location\n",
    "checkpoint_dir = Path('outputs/phase1_rgb_infonce')\n",
    "checkpoint_path = checkpoint_dir / 'checkpoint_latest.pt'\n",
    "config_path = checkpoint_dir / 'config.json'\n",
    "\n",
    "# Check if files exist\n",
    "if not checkpoint_path.exists():\n",
    "    print(f\"❌ Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"\\nPlease ensure you have a trained model in the outputs directory.\")\n",
    "    print(\"You can train a model using phase1_colab_training.ipynb\")\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "if not config_path.exists():\n",
    "    print(f\"❌ Config not found at {config_path}\")\n",
    "    raise FileNotFoundError(f\"Config not found: {config_path}\")\n",
    "\n",
    "print(f\"✓ Found checkpoint: {checkpoint_path}\")\n",
    "print(f\"✓ Found config: {config_path}\")\n",
    "\n",
    "# Load config\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Channels: {config['model']['num_channels']} (RGB)\")\n",
    "print(f\"  Grid size: {config['model']['grid_size']}x{config['model']['grid_size']}\")\n",
    "print(f\"  Hidden size: {config['model']['hidden_size']}\")\n",
    "\n",
    "# Create models\n",
    "encoder = create_model(config['model']).to(device)\n",
    "decoder = create_decoder(config['decoder']).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(f\"\\n✓ Loaded checkpoint from step {checkpoint['step']}\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"✓ Models loaded and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_attribution(latent, decoder, word_idx, predicted_token_id):\n",
    "    \"\"\"\n",
    "    Compute gradient-based attribution for a specific word.\n",
    "    \n",
    "    Args:\n",
    "        latent: [1, H, W, C] latent tensor (requires grad)\n",
    "        decoder: decoder model\n",
    "        word_idx: position of word in sequence\n",
    "        predicted_token_id: token ID that was predicted\n",
    "    \n",
    "    Returns:\n",
    "        heatmap: [H, W] numpy array of attribution scores\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    logits = decoder(latent)  # [1, seq_len, vocab_size]\n",
    "    \n",
    "    # Get logit for predicted token at this position\n",
    "    target_logit = logits[0, word_idx, predicted_token_id]\n",
    "    \n",
    "    # Backward pass\n",
    "    target_logit.backward(retain_graph=True)\n",
    "    \n",
    "    # Get gradient magnitude\n",
    "    grad = latent.grad.abs()  # [1, H, W, C]\n",
    "    \n",
    "    # Average across RGB channels to get [H, W] heatmap\n",
    "    heatmap = grad[0].mean(dim=-1)  # [H, W]\n",
    "    \n",
    "    return heatmap.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def latent_to_rgb(latent_tensor):\n",
    "    \"\"\"\n",
    "    Convert latent [H, W, C] to displayable RGB [H, W, 3].\n",
    "    \"\"\"\n",
    "    rgb = latent_tensor.cpu().numpy()\n",
    "    # Normalize from [-1.5, 1.5] to [0, 1]\n",
    "    rgb = (rgb + 1.5) / 3.0\n",
    "    rgb = np.clip(rgb, 0, 1)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def visualize_word_attributions(encoder, decoder, tokenizer, sentence, device):\n",
    "    \"\"\"\n",
    "    Generate visualization showing which latent pixels are important for each word.\n",
    "    \n",
    "    Display:\n",
    "    - Row 0: Original RGB latent (shown once)\n",
    "    - Row 1: Heatmap for each word\n",
    "    - Row 2: RGB + heatmap overlay for each word\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding='max_length', \n",
    "                      truncation=True, max_length=64)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Encode (no grad needed here)\n",
    "    with torch.no_grad():\n",
    "        latent_base = encoder(input_ids, attention_mask)  # [1, 32, 32, 3]\n",
    "        \n",
    "        # Decode to get predictions\n",
    "        logits = decoder(latent_base)\n",
    "        predicted_ids = logits.argmax(dim=-1)\n",
    "    \n",
    "    # Get predicted tokens (skip padding)\n",
    "    num_tokens = attention_mask[0].sum().item()\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_ids[0][:num_tokens])\n",
    "    predicted_token_ids = predicted_ids[0][:num_tokens].cpu().tolist()\n",
    "    \n",
    "    # Convert to displayable RGB\n",
    "    rgb = latent_to_rgb(latent_base[0])\n",
    "    \n",
    "    # Compute heatmaps for each word\n",
    "    print(f\"Computing attributions for {len(predicted_tokens)} tokens...\")\n",
    "    heatmaps = []\n",
    "    \n",
    "    for i, (token, token_id) in enumerate(zip(predicted_tokens, predicted_token_ids)):\n",
    "        # Create latent that requires grad for this computation\n",
    "        latent = latent_base.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Compute attribution\n",
    "        heatmap = compute_word_attribution(latent, decoder, i, token_id)\n",
    "        heatmaps.append(heatmap)\n",
    "        \n",
    "        # Clear gradients\n",
    "        if latent.grad is not None:\n",
    "            latent.grad.zero_()\n",
    "    \n",
    "    print(\"✓ Attributions computed\")\n",
    "    \n",
    "    # Create visualization\n",
    "    n_tokens = len(predicted_tokens)\n",
    "    fig = plt.figure(figsize=(n_tokens * 2, 8))\n",
    "    \n",
    "    # Row 0: Original RGB latent (spanning all columns)\n",
    "    ax_rgb = plt.subplot(3, 1, 1)\n",
    "    ax_rgb.imshow(rgb)\n",
    "    ax_rgb.set_title(f'RGB Latent for: \"{sentence}\"', fontsize=12, fontweight='bold')\n",
    "    ax_rgb.axis('off')\n",
    "    \n",
    "    # Rows 1-2: Heatmaps and overlays for each word\n",
    "    for i, (token, heatmap) in enumerate(zip(predicted_tokens, heatmaps)):\n",
    "        # Row 1: Heatmap\n",
    "        ax_heat = plt.subplot(3, n_tokens, n_tokens + i + 1)\n",
    "        im = ax_heat.imshow(heatmap, cmap='hot', interpolation='nearest')\n",
    "        ax_heat.set_title(f'{token}', fontsize=10)\n",
    "        ax_heat.axis('off')\n",
    "        \n",
    "        # Row 2: Overlay\n",
    "        ax_overlay = plt.subplot(3, n_tokens, 2 * n_tokens + i + 1)\n",
    "        ax_overlay.imshow(rgb)\n",
    "        ax_overlay.imshow(heatmap, cmap='hot', alpha=0.5, interpolation='nearest')\n",
    "        ax_overlay.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print reconstruction\n",
    "    reconstruction = tokenizer.decode(predicted_ids[0][:num_tokens], skip_special_tokens=True)\n",
    "    print(f\"\\nOriginal:       {sentence}\")\n",
    "    print(f\"Reconstruction: {reconstruction}\")\n",
    "    if sentence.strip() == reconstruction.strip():\n",
    "        print(\"✓ EXACT MATCH\")\n",
    "    \n",
    "    return fig, predicted_tokens, heatmaps\n",
    "\n",
    "print(\"✓ Attribution functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Analysis\n",
    "\n",
    "Enter a sentence to analyze which parts of the latent are responsible for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for sentence\n",
    "sentence = input(\"Enter a sentence to analyze: \")\n",
    "\n",
    "if not sentence.strip():\n",
    "    sentence = \"the red cube is under the yellow block\"  # Default example\n",
    "    print(f\"Using default: {sentence}\")\n",
    "\n",
    "# Generate visualization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "fig, tokens, heatmaps = visualize_word_attributions(encoder, decoder, tokenizer, sentence, device)\n",
    "print(\"=\"*70)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Tips\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "1. **Color words** (red, blue, yellow): Do they activate specific colored regions?\n",
    "2. **Spatial words** (under, right, left): Do they activate specific spatial patterns?\n",
    "3. **Object words** (cube, block, box): Do they have consistent activation patterns?\n",
    "4. **Function words** (the, is): Typically should have low/diffuse activation\n",
    "\n",
    "**Heatmap interpretation:**\n",
    "- **Bright (yellow/white)**: High importance - this pixel strongly influenced the word\n",
    "- **Dark (red/black)**: Low importance - this pixel didn't affect the word much\n",
    "\n",
    "**Questions to explore:**\n",
    "- Do different occurrences of \"the\" activate different regions?\n",
    "- Do color words consistently activate the same colored blobs?\n",
    "- Do spatial relations show positional patterns in the latent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try More Sentences\n",
    "\n",
    "Run the cell below multiple times with different sentences to explore patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences to try:\n",
    "examples = [\n",
    "    \"the red cube is under the yellow block\",\n",
    "    \"the blue box is right of the green sphere\",\n",
    "    \"the yellow block is on the red cube\",\n",
    "    \"the green cube is left of the blue box\",\n",
    "]\n",
    "\n",
    "print(\"Example sentences you can try:\")\n",
    "for i, ex in enumerate(examples, 1):\n",
    "    print(f\"{i}. {ex}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "sentence = input(\"Enter a sentence (or leave blank for random example): \")\n",
    "\n",
    "if not sentence.strip():\n",
    "    import random\n",
    "    sentence = random.choice(examples)\n",
    "    print(f\"Using random example: {sentence}\")\n",
    "\n",
    "fig, tokens, heatmaps = visualize_word_attributions(encoder, decoder, tokenizer, sentence, device)\n",
    "print(\"=\"*70)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
