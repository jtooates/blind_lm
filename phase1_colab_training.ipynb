{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# Phase 1: RGB Latent Training with InfoNCE on Google Colab\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jtooates/blind_lm/blob/main/phase1_colab_training.ipynb)\n\nThis notebook trains a text encoder to produce **3-channel RGB** latents (32×32×3) with spatially coherent colored patterns.\n\n**NEW:** Uses InfoNCE patch coherence loss for RGB spatial structure.\n\n**Goal**: Create colored images with local coherence and global diversity.\n\n**Training time**: ~2-3 hours on T4 GPU\n\n---\n\n## Setup Instructions\n\n1. **Runtime → Change runtime type → T4 GPU**\n2. Run all cells in order\n3. Checkpoints save to Google Drive automatically\n4. Results appear as RGB color images!"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(\"=\"*70)\n",
    "print(\"GPU Check\")\n",
    "print(\"=\"*70)\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No GPU found! Training will be very slow.\")\n",
    "    print(\"   Please go to Runtime → Change runtime type → T4 GPU\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to save checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory on Drive\n",
    "!mkdir -p /content/drive/MyDrive/blind_lm_outputs\n",
    "print(\"✓ Google Drive mounted\")\n",
    "print(\"✓ Checkpoints will save to: /content/drive/MyDrive/blind_lm_outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": "# Clone or update the repository\nimport os\n\nrepo_dir = 'blind_lm'\nrepo_url = 'https://github.com/jtooates/blind_lm.git'\n\nif os.path.exists(repo_dir):\n    print(\"Repository already exists. Pulling latest changes...\")\n    %cd blind_lm\n    !git pull origin main\n    print(\"✓ Repository updated to latest version\")\nelse:\n    print(\"Cloning repository...\")\n    !git clone {repo_url}\n    %cd blind_lm\n    print(\"✓ Repository cloned successfully\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Code is ready!\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q transformers scipy tqdm matplotlib\n",
    "\n",
    "# Suppress tokenizer warning\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "# Check if training data exists, generate if needed\n",
    "import os\n",
    "\n",
    "if not os.path.exists('train_sentences.txt'):\n",
    "    print(\"Generating training data (10,000 sentences)...\")\n",
    "    !python generate_sentences.py --num 10000 --complexity 1 --seed 42 --output train_sentences.txt\n",
    "    print(\"✓ Training data generated\")\n",
    "else:\n",
    "    print(\"✓ Training data already exists\")\n",
    "\n",
    "if not os.path.exists('val_sentences.txt'):\n",
    "    print(\"Generating validation data (1,000 sentences)...\")\n",
    "    !python generate_sentences.py --num 1000 --complexity 1 --seed 100 --output val_sentences.txt\n",
    "    print(\"✓ Validation data generated\")\n",
    "else:\n",
    "    print(\"✓ Validation data already exists\")\n",
    "\n",
    "# Show stats\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Data Statistics\")\n",
    "print(\"=\"*70)\n",
    "!wc -l train_sentences.txt val_sentences.txt\n",
    "\n",
    "print(\"\\nSample sentences:\")\n",
    "!head -5 train_sentences.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_config"
   },
   "outputs": [],
   "source": "# Create Colab-optimized config with RGB and InfoNCE losses\nimport json\n\nconfig = {\n    \"description\": \"RGB latent with InfoNCE patch coherence\",\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"output_dir\": \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce\",\n\n    \"model\": {\n        \"vocab_size\": 50257,\n        \"max_seq_len\": 64,\n        \"hidden_size\": 384,\n        \"num_layers\": 6,\n        \"num_heads\": 8,\n        \"ffn_size\": 1536,\n        \"dropout\": 0.1,\n        \"grid_size\": 32,\n        \"num_channels\": 3,  # RGB (3 channels)\n        \"use_rope\": True,\n        \"use_smooth_head\": False,\n        \"tokenizer_name\": \"gpt2\"\n    },\n\n    \"loss\": {\n        # InfoNCE-based losses\n        \"lambda_recon\": 5.0,           # Reconstruction loss weight\n        \"lambda_infonce\": 2.0,          # InfoNCE coherence weight\n        \"lambda_magnitude\": 5.0,        # Magnitude loss weight\n        \n        # InfoNCE parameters\n        \"infonce_patch_size\": 3,        # Patch size (3x3x3 for RGB)\n        \"infonce_num_samples\": 100,     # Number of anchor patches per image\n        \"infonce_temperature\": 1.0,     # Temperature for similarity\n        \"infonce_positive_radius\": 3.0, # Max distance for positive pairs (pixels)\n        \"infonce_negative_radius\": 11.0, # Min distance for negative pairs (pixels)\n        \"min_magnitude\": 0.3            # Minimum magnitude target\n    },\n\n    \"decoder\": {\n        \"vocab_size\": 50257,\n        \"max_seq_len\": 64,\n        \"hidden_size\": 384,\n        \"num_layers\": 4,\n        \"num_heads\": 8,\n        \"ffn_size\": 1536,\n        \"dropout\": 0.1,\n        \"use_rope\": True\n    },\n\n    \"training\": {\n        \"batch_size\": 128,  # Reduced for T4 GPU\n        \"lr\": 2e-4,\n        \"beta1\": 0.9,\n        \"beta2\": 0.95,\n        \"weight_decay\": 0.01,\n        \"warmup_steps\": 500,\n        \"num_epochs\": 1000,  # High limit - will stop at max_steps\n        \"max_steps\": 10000,  # Shorter initial training\n        \"ema_decay\": 0.999,\n        \"grad_clip\": 1.0,\n        \"blur_sigma\": 0.8,\n        \"blur_warmup_steps\": 0\n    },\n\n    \"data\": {\n        \"train_file\": \"../train_sentences.txt\",\n        \"val_file\": \"../val_sentences.txt\",\n        \"num_workers\": 2,  # Colab-optimized\n        \"file_format\": \"txt\"\n    },\n\n    \"eval\": {\n        \"eval_interval\": 500,\n        \"save_interval\": 2000,\n        \"num_fixed_sentences\": 16\n    }\n}\n\n# Save config\n!mkdir -p phase1/configs\nwith open('phase1/configs/phase1_colab.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(\"Configuration created:\")\nprint(f\"  Device: {config['device']}\")\nprint(f\"  Batch size: {config['training']['batch_size']}\")\nprint(f\"  Max steps: {config['training']['max_steps']}\")\nprint(f\"  Channels: {config['model']['num_channels']} (RGB)\")\nprint(f\"  Loss type: InfoNCE + Magnitude + Reconstruction\")\nprint(f\"  InfoNCE patch size: {config['loss']['infonce_patch_size']}x{config['loss']['infonce_patch_size']}\")\nprint(f\"  Output: {config['output_dir']}\")\nprint(\"\\n✓ Ready to train with RGB InfoNCE!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 4. Training\n",
    "\n",
    "This will take approximately **2-3 hours** on a T4 GPU.\n",
    "\n",
    "The training loop will:\n",
    "- Train for up to 50,000 steps (or 10 epochs)\n",
    "- Evaluate every 500 steps\n",
    "- Save checkpoints every 2,000 steps to Google Drive\n",
    "- Display progress bars and loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# Run training\n",
    "%cd phase1\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Starting Phase 1 Training\")\n",
    "print(\"=\"*70)\n",
    "print(\"This will take approximately 2-3 hours on T4 GPU\")\n",
    "print(\"You can monitor progress below...\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "!python train.py --config configs/phase1_colab.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor"
   },
   "source": [
    "## 5. Monitor Training (Optional)\n",
    "\n",
    "Run this cell **while training** to see intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_progress"
   },
   "outputs": [],
   "source": "# Check training progress\nimport os\nimport json\n\noutput_dir = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce\"\n\nif os.path.exists(output_dir):\n    print(\"Checkpoint files:\")\n    !ls -lh {output_dir}/checkpoint_*.pt\n    \n    # Try to load latest checkpoint and show metrics\n    latest = os.path.join(output_dir, \"checkpoint_latest.pt\")\n    if os.path.exists(latest):\n        import torch\n        checkpoint = torch.load(latest, map_location='cpu')\n        print(f\"\\nCurrent step: {checkpoint['step']}\")\n        print(f\"Current epoch: {checkpoint['epoch']}\")\n        \n        if 'metrics_history' in checkpoint and checkpoint['metrics_history']:\n            latest_metrics = checkpoint['metrics_history'][-1]\n            print(f\"\\nLatest evaluation metrics:\")\n            print(f\"  Total loss: {latest_metrics.get('eval_loss', 'N/A')}\")\n            if 'loss_components' in latest_metrics:\n                print(f\"\\nLoss components:\")\n                for name, value in latest_metrics['loss_components'].items():\n                    print(f\"  {name}: {value:.4f}\")\nelse:\n    print(\"No checkpoints found yet. Training may not have started.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Evaluation and Visualization\n",
    "\n",
    "After training completes, generate comprehensive evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_report"
   },
   "outputs": [],
   "source": "# Simple evaluation - just show some sample reconstructions\nimport sys\nsys.path.append('..')\n\nfrom model import create_model\nfrom decoder import TextDecoder\nfrom transformers import AutoTokenizer\nimport torch\nimport json\nimport os\n\nprint(\"Generating sample reconstructions...\")\n\noutput_dir = '/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce'\n\n# Load config\nwith open(os.path.join(output_dir, 'config.json')) as f:\n    config = json.load(f)\n\n# Load models\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nencoder = create_model(config['model']).to(device)\ndecoder = TextDecoder(**config['decoder']).to(device)\n\n# Load checkpoint\ncheckpoint_path = os.path.join(output_dir, 'checkpoint_latest.pt')\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nencoder.load_state_dict(checkpoint['encoder_state_dict'])\ndecoder.load_state_dict(checkpoint['decoder_state_dict'])\nencoder.eval()\ndecoder.eval()\n\nprint(f'Loaded checkpoint from step {checkpoint[\"step\"]}')\n\n# Create tokenizer\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\n# Test sentences\ntest_sentences = [\n    \"The cat sits on the mat.\",\n    \"A red ball bounces high.\",\n    \"Birds fly in the blue sky.\",\n    \"The dog runs through the park.\",\n    \"A small box rests near a tree.\"\n]\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"SAMPLE RECONSTRUCTIONS\")\nprint(\"=\"*70)\n\nwith torch.no_grad():\n    for sentence in test_sentences:\n        # Tokenize\n        inputs = tokenizer(sentence, return_tensors='pt', padding='max_length', \n                         truncation=True, max_length=64)\n        input_ids = inputs['input_ids'].to(device)\n        attention_mask = inputs['attention_mask'].to(device)\n        \n        # Encode\n        latent = encoder(input_ids, attention_mask)\n        \n        # Decode\n        logits = decoder(latent, input_ids, attention_mask)\n        predicted_ids = torch.argmax(logits, dim=-1)\n        \n        # Decode text\n        reconstruction = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n        \n        print(f\"\\nOriginal:       {sentence}\")\n        print(f\"Reconstruction: {reconstruction}\")\n        \n        # Check exact match\n        if sentence.strip() == reconstruction.strip():\n            print(\"✓ EXACT MATCH\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_results"
   },
   "outputs": [],
   "source": "# Display evaluation results for RGB latents\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np\nimport json\nimport os\n\neval_dir = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/eval_report\"\n\nprint(\"=\"*70)\nprint(\"RGB LATENT EVALUATION RESULTS\")\nprint(\"=\"*70)\n\n# Check if we can visualize the RGB latents directly\ncheckpoint_path = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/checkpoint_latest.pt\"\nif os.path.exists(checkpoint_path):\n    # Load the checkpoint to get some sample latents\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    \n    print(f\"\\nCheckpoint Info:\")\n    print(f\"  Step: {checkpoint['step']}\")\n    print(f\"  Epoch: {checkpoint['epoch']}\")\n    \n    # Display loss components if available\n    if 'metrics_history' in checkpoint and checkpoint['metrics_history']:\n        latest = checkpoint['metrics_history'][-1]\n        print(f\"\\nLatest Loss Components:\")\n        if 'loss_components' in latest:\n            for name, value in latest['loss_components'].items():\n                print(f\"  {name}: {value:.4f}\")\n\n# Try to display RGB visualizations if they exist\nprint(\"\\n\" + \"=\"*70)\nprint(\"RGB VISUALIZATIONS\")\nprint(\"=\"*70)\n\n# Helper function to convert latent to RGB for display\ndef latent_to_rgb(latent_tensor):\n    \"\"\"Convert [H, W, 3] tensor to displayable RGB, normalized to [0, 1]\"\"\"\n    rgb = latent_tensor.cpu().numpy()\n    # Normalize from [-1.5, 1.5] to [0, 1]\n    rgb = (rgb + 1.5) / 3.0\n    rgb = np.clip(rgb, 0, 1)\n    return rgb\n\n# Generate a simple visualization of the RGB latents\ntry:\n    from model import create_model\n    from transformers import AutoTokenizer\n    \n    # Load model and tokenizer\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Load config\n    config_path = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/config.json\"\n    if os.path.exists(config_path):\n        with open(config_path) as f:\n            config = json.load(f)\n        \n        encoder = create_model(config['model']).to(device)\n        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n        encoder.eval()\n        \n        tokenizer = AutoTokenizer.from_pretrained('gpt2')\n        tokenizer.pad_token = tokenizer.eos_token\n        \n        # Generate some sample sentences\n        test_sentences = [\n            \"The cat sits on the mat.\",\n            \"A red ball bounces high.\",\n            \"Birds fly in the blue sky.\",\n            \"The dog runs through the park.\"\n        ]\n        \n        # Create figure for RGB images\n        fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n        axes = axes.flatten()\n        \n        with torch.no_grad():\n            for i, sentence in enumerate(test_sentences):\n                # Tokenize\n                inputs = tokenizer(sentence, return_tensors='pt', padding='max_length', \n                                 truncation=True, max_length=64)\n                input_ids = inputs['input_ids'].to(device)\n                attention_mask = inputs['attention_mask'].to(device)\n                \n                # Generate latent\n                latent = encoder(input_ids, attention_mask)  # [1, 32, 32, 3]\n                \n                # Convert to RGB for display\n                rgb = latent_to_rgb(latent[0])\n                \n                # Display\n                axes[i].imshow(rgb)\n                axes[i].set_title(f'\"{sentence[:20]}...\"' if len(sentence) > 20 else f'\"{sentence}\"', \n                                 fontsize=8)\n                axes[i].axis('off')\n        \n        plt.suptitle('RGB Latents (3 channels)', fontsize=12)\n        plt.tight_layout()\n        plt.savefig('/content/rgb_samples.png', dpi=150, bbox_inches='tight')\n        plt.show()\n        \n        print(\"✓ RGB visualization generated!\")\n        \nexcept Exception as e:\n    print(f\"Could not generate live visualization: {e}\")\n    print(\"This is normal if training hasn't completed yet.\")\n\n# Show any saved visualizations\nif os.path.exists(eval_dir):\n    viz_files = os.listdir(eval_dir)\n    if viz_files:\n        print(f\"\\nSaved visualizations in {eval_dir}:\")\n        for file in viz_files:\n            if file.endswith('.png'):\n                print(f\"  - {file}\")\n                display(Image(os.path.join(eval_dir, file)))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interpret"
   },
   "source": "## 7. Interpret Results\n\n### What to Expect with RGB InfoNCE\n\nWith **3-channel RGB** output and **InfoNCE patch coherence**, you should see:\n\n**RGB Color Images**:\n- Full-color visualizations showing all 3 channels (R, G, B)\n- Spatially coherent colored patterns (nearby regions have similar colors)\n- Local smoothness with global diversity (different parts of image can have different colors)\n- Different sentences produce distinct colored patterns\n\n**InfoNCE Coherence Effects**:\n- Nearby patches (within positive_radius ~3 pixels) will be similar in color\n- Distant patches (beyond negative_radius ~11 pixels) will differ\n- Creates smooth, blob-like colored regions\n- Prevents noisy, scattered pixels\n\n**Reconstruction Quality**:\n- High reconstruction accuracy (target >40% exact match)\n- Semantically similar sentences produce similar RGB patterns\n- Different sentences produce distinct RGB patterns\n- Text decoder can reconstruct original text from RGB latent\n\n### Loss Components to Monitor\n\n- **Reconstruction loss**: Should decrease steadily (target < 1.0)\n  - Measures how well the text decoder reconstructs the input\n  \n- **InfoNCE loss**: Should stabilize after initial decrease\n  - Measures spatial coherence of RGB patches\n  - Lower = more coherent colored regions\n  \n- **Magnitude loss**: Should approach zero as training progresses\n  - Ensures latent doesn't collapse to all zeros\n  - Maintains meaningful signal strength\n\n### Training Tips\n\n- **If patterns are too noisy**: Increase `lambda_infonce` or decrease `infonce_temperature`\n- **If patterns are too uniform**: Decrease `lambda_infonce` or increase `infonce_positive_radius`\n- **If reconstruction is poor**: Increase `lambda_recon`\n- **If latents collapse**: Increase `lambda_magnitude`"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 8. Download Checkpoints (Optional)\n",
    "\n",
    "Download the final checkpoint and visualizations to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": "# Create a zip file with important results\nimport shutil\nimport os\n\noutput_dir = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce\"\nzip_path = \"/content/phase1_rgb_infonce_results.zip\"\n\nprint(\"Creating results archive...\")\n\n# Create temporary directory\ntemp_dir = \"/content/phase1_results_temp\"\nos.makedirs(temp_dir, exist_ok=True)\n\n# Copy important files\nfiles_to_include = [\n    \"config.json\",\n    \"checkpoint_latest.pt\"\n]\n\nfor file in files_to_include:\n    src = os.path.join(output_dir, file)\n    if os.path.exists(src):\n        dst_dir = os.path.join(temp_dir, os.path.dirname(file))\n        os.makedirs(dst_dir, exist_ok=True)\n        shutil.copy2(src, os.path.join(temp_dir, file))\n        print(f\"  ✓ {file}\")\n    else:\n        print(f\"  ✗ {file} not found\")\n\n# Create zip\nshutil.make_archive('/content/phase1_rgb_infonce_results', 'zip', temp_dir)\n\n# Download\nfrom google.colab import files\nprint(\"\\nDownloading...\")\nfiles.download('/content/phase1_rgb_infonce_results.zip')\n\nprint(\"\\n✓ Download complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "After Phase 1 passes:\n",
    "\n",
    "1. **Phase 2**: Add semantic meaning via contrastive learning\n",
    "   - Paraphrases should produce similar latents\n",
    "   - Counterfactuals should produce different latents\n",
    "\n",
    "2. **Phase 3**: Spatial jitter robustness\n",
    "   - Latents should be invariant to small shifts\n",
    "\n",
    "3. **Phase 4**: Add text decoder\n",
    "   - Reconstruct text from latent\n",
    "\n",
    "4. **Phase 5**: Round-trip generation\n",
    "   - Generate paraphrases without copying\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the [project documentation](https://github.com/jtooates/blind_lm)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Phase 1: Image-like Latent Training",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}