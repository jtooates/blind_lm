{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Phase 1: Image-like Latent Training on Google Colab\n",
    "\n",
    "This notebook trains a text encoder to produce 2D visual latents (32×32×6) with natural image statistics.\n",
    "\n",
    "**Goal**: Make the latent grid look image-like using only analytic priors (no semantic understanding).\n",
    "\n",
    "**Training time**: ~2-3 hours on T4 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime → Change runtime type → T4 GPU**\n",
    "2. Run all cells in order\n",
    "3. Checkpoints save to Google Drive automatically\n",
    "4. Results appear at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(\"=\"*70)\n",
    "print(\"GPU Check\")\n",
    "print(\"=\"*70)\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No GPU found! Training will be very slow.\")\n",
    "    print(\"   Please go to Runtime → Change runtime type → T4 GPU\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to save checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory on Drive\n",
    "!mkdir -p /content/drive/MyDrive/blind_lm_outputs\n",
    "print(\"✓ Google Drive mounted\")\n",
    "print(\"✓ Checkpoints will save to: /content/drive/MyDrive/blind_lm_outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/jtooates/blind_lm.git\n",
    "%cd blind_lm\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Repository cloned successfully!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q transformers scipy tqdm matplotlib\n",
    "\n",
    "# Suppress tokenizer warning\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "# Check if training data exists, generate if needed\n",
    "import os\n",
    "\n",
    "if not os.path.exists('train_sentences.txt'):\n",
    "    print(\"Generating training data (10,000 sentences)...\")\n",
    "    !python generate_sentences.py --num 10000 --complexity 1 --seed 42 --output train_sentences.txt\n",
    "    print(\"✓ Training data generated\")\n",
    "else:\n",
    "    print(\"✓ Training data already exists\")\n",
    "\n",
    "if not os.path.exists('val_sentences.txt'):\n",
    "    print(\"Generating validation data (1,000 sentences)...\")\n",
    "    !python generate_sentences.py --num 1000 --complexity 1 --seed 100 --output val_sentences.txt\n",
    "    print(\"✓ Validation data generated\")\n",
    "else:\n",
    "    print(\"✓ Validation data already exists\")\n",
    "\n",
    "# Show stats\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Data Statistics\")\n",
    "print(\"=\"*70)\n",
    "!wc -l train_sentences.txt val_sentences.txt\n",
    "\n",
    "print(\"\\nSample sentences:\")\n",
    "!head -5 train_sentences.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_config"
   },
   "outputs": [],
   "source": [
    "# Create Colab-optimized config\n",
    "import json\n",
    "\n",
    "config = {\n",
    "    \"description\": \"Colab training configuration with T4 GPU optimizations\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"output_dir\": \"/content/drive/MyDrive/blind_lm_outputs/phase1\",\n",
    "\n",
    "    \"model\": {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"max_seq_len\": 64,\n",
    "        \"hidden_size\": 384,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_heads\": 8,\n",
    "        \"ffn_size\": 1536,\n",
    "        \"dropout\": 0.1,\n",
    "        \"grid_size\": 32,\n",
    "        \"num_channels\": 6,\n",
    "        \"use_rope\": True,\n",
    "        \"use_smooth_head\": True,\n",
    "        \"tokenizer_name\": \"gpt2\"\n",
    "    },\n",
    "\n",
    "    \"loss\": {\n",
    "        \"lambda_spec\": 0.5,\n",
    "        \"lambda_tv\": 0.1,\n",
    "        \"lambda_wav\": 0.1,\n",
    "        \"lambda_kurt\": 0.05,\n",
    "        \"lambda_cov\": 0.05,\n",
    "        \"lambda_var\": 0.05\n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"batch_size\": 128,  # Reduced for T4 GPU\n",
    "        \"lr\": 2e-4,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.95,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"warmup_steps\": 1000,\n",
    "        \"num_epochs\": 10,\n",
    "        \"max_steps\": 50000,\n",
    "        \"ema_decay\": 0.999,\n",
    "        \"grad_clip\": 1.0,\n",
    "        \"blur_sigma\": 0.8,\n",
    "        \"blur_warmup_steps\": 2000\n",
    "    },\n",
    "\n",
    "    \"data\": {\n",
    "        \"train_file\": \"../train_sentences.txt\",\n",
    "        \"val_file\": \"../val_sentences.txt\",\n",
    "        \"num_workers\": 2,  # Colab-optimized\n",
    "        \"file_format\": \"txt\"\n",
    "    },\n",
    "\n",
    "    \"eval\": {\n",
    "        \"eval_interval\": 500,\n",
    "        \"save_interval\": 2000,\n",
    "        \"num_fixed_sentences\": 16\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save config\n",
    "!mkdir -p phase1/configs\n",
    "with open('phase1/configs/phase1_colab.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration created:\")\n",
    "print(f\"  Device: {config['device']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Max steps: {config['training']['max_steps']}\")\n",
    "print(f\"  Output: {config['output_dir']}\")\n",
    "print(\"\\n✓ Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 4. Training\n",
    "\n",
    "This will take approximately **2-3 hours** on a T4 GPU.\n",
    "\n",
    "The training loop will:\n",
    "- Train for up to 50,000 steps (or 10 epochs)\n",
    "- Evaluate every 500 steps\n",
    "- Save checkpoints every 2,000 steps to Google Drive\n",
    "- Display progress bars and loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# Run training\n",
    "%cd phase1\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Starting Phase 1 Training\")\n",
    "print(\"=\"*70)\n",
    "print(\"This will take approximately 2-3 hours on T4 GPU\")\n",
    "print(\"You can monitor progress below...\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "!python train.py --config configs/phase1_colab.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor"
   },
   "source": [
    "## 5. Monitor Training (Optional)\n",
    "\n",
    "Run this cell **while training** to see intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_progress"
   },
   "outputs": [],
   "source": [
    "# Check training progress\n",
    "import os\n",
    "import json\n",
    "\n",
    "output_dir = \"/content/drive/MyDrive/blind_lm_outputs/phase1\"\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    print(\"Checkpoint files:\")\n",
    "    !ls -lh {output_dir}/checkpoint_*.pt\n",
    "    \n",
    "    # Try to load latest checkpoint and show metrics\n",
    "    latest = os.path.join(output_dir, \"checkpoint_latest.pt\")\n",
    "    if os.path.exists(latest):\n",
    "        import torch\n",
    "        checkpoint = torch.load(latest, map_location='cpu')\n",
    "        print(f\"\\nCurrent step: {checkpoint['step']}\")\n",
    "        print(f\"Current epoch: {checkpoint['epoch']}\")\n",
    "        \n",
    "        if 'metrics_history' in checkpoint and checkpoint['metrics_history']:\n",
    "            latest_metrics = checkpoint['metrics_history'][-1]\n",
    "            print(f\"\\nLatest evaluation metrics:\")\n",
    "            print(f\"  Loss: {latest_metrics.get('eval_loss', 'N/A')}\")\n",
    "            if 'eval_metrics' in latest_metrics:\n",
    "                print(f\"  Mean slope: {latest_metrics['eval_metrics'].get('mean_slope', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet. Training may not have started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Evaluation and Visualization\n",
    "\n",
    "After training completes, generate comprehensive evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_report"
   },
   "outputs": [],
   "source": [
    "# Generate evaluation report\n",
    "print(\"Generating evaluation report...\")\n",
    "\n",
    "eval_code = \"\"\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from visualize import create_evaluation_report\n",
    "from model import create_model\n",
    "from dataloader import create_fixed_eval_set\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/blind_lm_outputs/phase1'\n",
    "\n",
    "# Load config\n",
    "with open(os.path.join(output_dir, 'config.json')) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = create_model(config['model']).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = os.path.join(output_dir, 'checkpoint_latest.pt')\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f'Loaded checkpoint from step {checkpoint[\\\"step\\\"]}')\n",
    "\n",
    "# Create eval set\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "eval_set = create_fixed_eval_set(tokenizer)\n",
    "\n",
    "# Generate report\n",
    "eval_report_dir = os.path.join(output_dir, 'eval_report')\n",
    "summary = create_evaluation_report(\n",
    "    model, eval_set, device=device, output_dir=eval_report_dir\n",
    ")\n",
    "\n",
    "print('\\nEvaluation complete!')\n",
    "print(f'Report saved to: {eval_report_dir}')\n",
    "\"\"\"\n",
    "\n",
    "with open('/tmp/eval_script.py', 'w') as f:\n",
    "    f.write(eval_code)\n",
    "\n",
    "!python /tmp/eval_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_results"
   },
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "from IPython.display import Image, display, JSON\n",
    "import json\n",
    "import os\n",
    "\n",
    "eval_dir = \"/content/drive/MyDrive/blind_lm_outputs/phase1/eval_report\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show summary\n",
    "summary_path = os.path.join(eval_dir, \"evaluation_summary.json\")\n",
    "if os.path.exists(summary_path):\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"  Mean slope: {summary['mean_slope']:.2f}\")\n",
    "    print(f\"  Slopes in target range [1.5, 2.5]: {summary['slopes_in_range']}/6\")\n",
    "    print(f\"  Gradient kurtosis (h): {summary['kurtosis_h']:.2f}\")\n",
    "    print(f\"  Gradient kurtosis (w): {summary['kurtosis_w']:.2f}\")\n",
    "    \n",
    "    print(\"\\nSlopes per channel:\")\n",
    "    for i, slope in enumerate(summary['slopes']):\n",
    "        in_range = \"✓\" if 1.5 <= slope <= 2.5 else \"✗\"\n",
    "        print(f\"  Channel {i+1}: {slope:.2f} {in_range}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display visualizations\n",
    "viz_files = [\n",
    "    (\"power_spectra.png\", \"Power Spectra (target: α ∈ [1.5, 2.5])\"),\n",
    "    (\"channel_montage.png\", \"Channel Montage (16 sentences × 6 channels)\"),\n",
    "    (\"gradient_histograms.png\", \"Gradient Distributions (target: kurtosis > 3)\"),\n",
    "    (\"channel_covariance.png\", \"Channel Covariance (target: diagonal)\")\n",
    "]\n",
    "\n",
    "for filename, title in viz_files:\n",
    "    path = os.path.join(eval_dir, filename)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\n{title}:\")\n",
    "        display(Image(path))\n",
    "    else:\n",
    "        print(f\"\\n⚠️  {filename} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interpret"
   },
   "source": [
    "## 7. Interpret Results\n",
    "\n",
    "### Pass Criteria\n",
    "\n",
    "Phase 1 **PASSES** if:\n",
    "- ✅ ≥ 4/6 channels have α ∈ [1.5, 2.5]\n",
    "- ✅ Gradient kurtosis > 3\n",
    "- ✅ Channel covariance is approximately diagonal\n",
    "- ✅ Visual montage shows smooth blobs/edges (no checkerboards)\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "**Power Spectra**: Should show slopes between 1.5-2.5 (natural images are ~2)\n",
    "\n",
    "**Channel Montage**: Should show varied patterns with:\n",
    "- Smooth regions\n",
    "- Clear edges\n",
    "- Different patterns per channel\n",
    "- NO checkerboard artifacts\n",
    "\n",
    "**Gradient Histograms**: Should show heavy tails (kurtosis > 3)\n",
    "\n",
    "**Channel Covariance**: Should be near-diagonal (channels independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 8. Download Checkpoints (Optional)\n",
    "\n",
    "Download the final checkpoint and visualizations to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Create a zip file with important results\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "output_dir = \"/content/drive/MyDrive/blind_lm_outputs/phase1\"\n",
    "zip_path = \"/content/phase1_results.zip\"\n",
    "\n",
    "print(\"Creating results archive...\")\n",
    "\n",
    "# Create temporary directory\n",
    "temp_dir = \"/content/phase1_results_temp\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Copy important files\n",
    "files_to_include = [\n",
    "    \"config.json\",\n",
    "    \"checkpoint_latest.pt\",\n",
    "    \"eval_report/power_spectra.png\",\n",
    "    \"eval_report/channel_montage.png\",\n",
    "    \"eval_report/gradient_histograms.png\",\n",
    "    \"eval_report/channel_covariance.png\",\n",
    "    \"eval_report/evaluation_summary.json\"\n",
    "]\n",
    "\n",
    "for file in files_to_include:\n",
    "    src = os.path.join(output_dir, file)\n",
    "    if os.path.exists(src):\n",
    "        dst_dir = os.path.join(temp_dir, os.path.dirname(file))\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        shutil.copy2(src, os.path.join(temp_dir, file))\n",
    "        print(f\"  ✓ {file}\")\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive('/content/phase1_results', 'zip', temp_dir)\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "print(\"\\nDownloading...\")\n",
    "files.download('/content/phase1_results.zip')\n",
    "\n",
    "print(\"\\n✓ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "After Phase 1 passes:\n",
    "\n",
    "1. **Phase 2**: Add semantic meaning via contrastive learning\n",
    "   - Paraphrases should produce similar latents\n",
    "   - Counterfactuals should produce different latents\n",
    "\n",
    "2. **Phase 3**: Spatial jitter robustness\n",
    "   - Latents should be invariant to small shifts\n",
    "\n",
    "3. **Phase 4**: Add text decoder\n",
    "   - Reconstruct text from latent\n",
    "\n",
    "4. **Phase 5**: Round-trip generation\n",
    "   - Generate paraphrases without copying\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the [project documentation](https://github.com/jtooates/blind_lm)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Phase 1: Image-like Latent Training",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
