{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Phase 1: RGB Latent Training with Mumford-Shah Loss on Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jtooates/blind_lm/blob/main/phase1_colab_training.ipynb)\n",
    "\n",
    "This notebook trains a text encoder to produce **3-channel RGB** latents (32×32×3) with piecewise constant regions.\n",
    "\n",
    "**NEW:** Uses Mumford-Shah loss for smooth regions with sharp boundaries + jittering to prevent speckles.\n",
    "\n",
    "**Goal**: Create colored images with smooth homogeneous regions separated by boundaries.\n",
    "\n",
    "**Training time**: ~2-3 hours on T4 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime → Change runtime type → T4 GPU**\n",
    "2. Run all cells in order\n",
    "3. Checkpoints save to Google Drive automatically\n",
    "4. Results appear as RGB color images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU Check\n",
      "======================================================================\n",
      "GPU Available: False\n",
      "⚠️  WARNING: No GPU found! Training will be very slow.\n",
      "   Please go to Runtime → Change runtime type → T4 GPU\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(\"=\"*70)\n",
    "print(\"GPU Check\")\n",
    "print(\"=\"*70)\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No GPU found! Training will be very slow.\")\n",
    "    print(\"   Please go to Runtime → Change runtime type → T4 GPU\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mount Google Drive to save checkpoints\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create output directory on Drive\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to save checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory on Drive\n",
    "!mkdir -p /content/drive/MyDrive/blind_lm_outputs\n",
    "print(\"✓ Google Drive mounted\")\n",
    "print(\"✓ Checkpoints will save to: /content/drive/MyDrive/blind_lm_outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning repository...\n",
      "Cloning into 'blind_lm'...\n",
      "remote: Enumerating objects: 373, done.\u001b[K\n",
      "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
      "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
      "remote: Total 373 (delta 84), reused 81 (delta 33), pack-reused 240 (from 1)\u001b[K\n",
      "Receiving objects: 100% (373/373), 3.89 MiB | 20.14 MiB/s, done.\n",
      "Resolving deltas: 100% (228/228), done.\n",
      "/Users/oates/Desktop/mydocuments/projects/current/blind_lm/blind_lm\n",
      "✓ Repository cloned successfully\n",
      "\n",
      "======================================================================\n",
      "Code is ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Clone or update the repository\n",
    "import os\n",
    "\n",
    "repo_dir = 'blind_lm'\n",
    "repo_url = 'https://github.com/jtooates/blind_lm.git'\n",
    "\n",
    "if os.path.exists(repo_dir):\n",
    "    print(\"Repository already exists. Pulling latest changes...\")\n",
    "    %cd blind_lm\n",
    "    !git pull origin main\n",
    "    print(\"✓ Repository updated to latest version\")\n",
    "else:\n",
    "    print(\"Cloning repository...\")\n",
    "    !git clone {repo_url}\n",
    "    %cd blind_lm\n",
    "    print(\"✓ Repository cloned successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Code is ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "✓ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q transformers scipy tqdm matplotlib\n",
    "\n",
    "# Suppress tokenizer warning\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data (10,000 sentences)...\n",
      "Generated 100/10000 sentences...\n",
      "Generated 200/10000 sentences...\n",
      "Generated 300/10000 sentences...\n",
      "Generated 400/10000 sentences...\n",
      "Generated 500/10000 sentences...\n",
      "Generated 600/10000 sentences...\n",
      "Generated 700/10000 sentences...\n",
      "Generated 800/10000 sentences...\n",
      "Generated 900/10000 sentences...\n",
      "Generated 1000/10000 sentences...\n",
      "Generated 1100/10000 sentences...\n",
      "Generated 1200/10000 sentences...\n",
      "Generated 1300/10000 sentences...\n",
      "Generated 1400/10000 sentences...\n",
      "Generated 1500/10000 sentences...\n",
      "Generated 1600/10000 sentences...\n",
      "Generated 1700/10000 sentences...\n",
      "Generated 1800/10000 sentences...\n",
      "Generated 1900/10000 sentences...\n",
      "Generated 2000/10000 sentences...\n",
      "Generated 2100/10000 sentences...\n",
      "Generated 2200/10000 sentences...\n",
      "Generated 2300/10000 sentences...\n",
      "Generated 2400/10000 sentences...\n",
      "Generated 2500/10000 sentences...\n",
      "Generated 2600/10000 sentences...\n",
      "Generated 2700/10000 sentences...\n",
      "Generated 2800/10000 sentences...\n",
      "Generated 2900/10000 sentences...\n",
      "Generated 3000/10000 sentences...\n",
      "Generated 3100/10000 sentences...\n",
      "Generated 3200/10000 sentences...\n",
      "Generated 3300/10000 sentences...\n",
      "Generated 3400/10000 sentences...\n",
      "Generated 3500/10000 sentences...\n",
      "Generated 3600/10000 sentences...\n",
      "Generated 3700/10000 sentences...\n",
      "Generated 3800/10000 sentences...\n",
      "Generated 3900/10000 sentences...\n",
      "Generated 4000/10000 sentences...\n",
      "Generated 4100/10000 sentences...\n",
      "Generated 4200/10000 sentences...\n",
      "Generated 4300/10000 sentences...\n",
      "Generated 4400/10000 sentences...\n",
      "Generated 4500/10000 sentences...\n",
      "Generated 4600/10000 sentences...\n",
      "Generated 4700/10000 sentences...\n",
      "Generated 4800/10000 sentences...\n",
      "Generated 4900/10000 sentences...\n",
      "Generated 5000/10000 sentences...\n",
      "Generated 5100/10000 sentences...\n",
      "Generated 5200/10000 sentences...\n",
      "Generated 5300/10000 sentences...\n",
      "Generated 5400/10000 sentences...\n",
      "Generated 5500/10000 sentences...\n",
      "Generated 5600/10000 sentences...\n",
      "Generated 5700/10000 sentences...\n",
      "Generated 5800/10000 sentences...\n",
      "Generated 5900/10000 sentences...\n",
      "Generated 6000/10000 sentences...\n",
      "Generated 6100/10000 sentences...\n",
      "Generated 6200/10000 sentences...\n",
      "Generated 6300/10000 sentences...\n",
      "Generated 6400/10000 sentences...\n",
      "Generated 6500/10000 sentences...\n",
      "Generated 6600/10000 sentences...\n",
      "Generated 6700/10000 sentences...\n",
      "Generated 6800/10000 sentences...\n",
      "Generated 6900/10000 sentences...\n",
      "Generated 7000/10000 sentences...\n",
      "Generated 7100/10000 sentences...\n",
      "Generated 7200/10000 sentences...\n",
      "Generated 7300/10000 sentences...\n",
      "Generated 7400/10000 sentences...\n",
      "Generated 7500/10000 sentences...\n",
      "Generated 7600/10000 sentences...\n",
      "Generated 7700/10000 sentences...\n",
      "Generated 7800/10000 sentences...\n",
      "Generated 7900/10000 sentences...\n",
      "Generated 8000/10000 sentences...\n",
      "Generated 8100/10000 sentences...\n",
      "Generated 8200/10000 sentences...\n",
      "Generated 8300/10000 sentences...\n",
      "Generated 8400/10000 sentences...\n",
      "Generated 8500/10000 sentences...\n",
      "Generated 8600/10000 sentences...\n",
      "Generated 8700/10000 sentences...\n",
      "Generated 8800/10000 sentences...\n",
      "Generated 8900/10000 sentences...\n",
      "Generated 9000/10000 sentences...\n",
      "Generated 9100/10000 sentences...\n",
      "Generated 9200/10000 sentences...\n",
      "Generated 9300/10000 sentences...\n",
      "Generated 9400/10000 sentences...\n",
      "Generated 9500/10000 sentences...\n",
      "Generated 9600/10000 sentences...\n",
      "Generated 9700/10000 sentences...\n",
      "Generated 9800/10000 sentences...\n",
      "Generated 9900/10000 sentences...\n",
      "Generated 10000/10000 sentences...\n",
      "Wrote 10000 sentences to train_sentences.txt\n",
      "✓ Training data generated\n",
      "Generating validation data (1,000 sentences)...\n",
      "Generated 100/1000 sentences...\n",
      "Generated 200/1000 sentences...\n",
      "Generated 300/1000 sentences...\n",
      "Generated 400/1000 sentences...\n",
      "Generated 500/1000 sentences...\n",
      "Generated 600/1000 sentences...\n",
      "Generated 700/1000 sentences...\n",
      "Generated 800/1000 sentences...\n",
      "Generated 900/1000 sentences...\n",
      "Generated 1000/1000 sentences...\n",
      "Wrote 1000 sentences to val_sentences.txt\n",
      "✓ Validation data generated\n",
      "\n",
      "======================================================================\n",
      "Data Statistics\n",
      "======================================================================\n",
      "   10000 train_sentences.txt\n",
      "    1000 val_sentences.txt\n",
      "   11000 total\n",
      "\n",
      "Sample sentences:\n",
      "the green block is next to the blue block\n",
      "the red block is under the blue block\n",
      "the blue block is on the green cube\n",
      "the green cube is left of the red cube\n",
      "the blue box is under the green box\n"
     ]
    }
   ],
   "source": [
    "# Check if training data exists, generate if needed\n",
    "import os\n",
    "\n",
    "if not os.path.exists('train_sentences.txt'):\n",
    "    print(\"Generating training data (10,000 sentences)...\")\n",
    "    !python generate_sentences.py --num 10000 --complexity 1 --seed 42 --output train_sentences.txt\n",
    "    print(\"✓ Training data generated\")\n",
    "else:\n",
    "    print(\"✓ Training data already exists\")\n",
    "\n",
    "if not os.path.exists('val_sentences.txt'):\n",
    "    print(\"Generating validation data (1,000 sentences)...\")\n",
    "    !python generate_sentences.py --num 1000 --complexity 1 --seed 100 --output val_sentences.txt\n",
    "    print(\"✓ Validation data generated\")\n",
    "else:\n",
    "    print(\"✓ Validation data already exists\")\n",
    "\n",
    "# Show stats\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Data Statistics\")\n",
    "print(\"=\"*70)\n",
    "!wc -l train_sentences.txt val_sentences.txt\n",
    "\n",
    "print(\"\\nSample sentences:\")\n",
    "!head -5 train_sentences.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "create_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration created:\n",
      "  Device: cpu\n",
      "  Batch size: 64\n",
      "  Max steps: 10000\n",
      "  Channels: 3 (RGB)\n",
      "\n",
      "Loss components:\n",
      "  - Reconstruction: 5.0\n",
      "  - Magnitude: 5.0\n",
      "  - Mumford-Shah: 5.0\n",
      "\n",
      "Mumford-Shah settings:\n",
      "  - Alpha (smoothness): 5.0\n",
      "  - Beta (boundary): 0.0\n",
      "\n",
      "Jittering:\n",
      "  - Jitter std: 0.1\n",
      "\n",
      "Output: /content/drive/MyDrive/blind_lm_outputs/phase1_rgb_mumford_shah\n",
      "\n",
      "✓ Ready to train with Mumford-Shah loss for smooth regions!\n"
     ]
    }
   ],
   "source": [
    "# Create Colab-optimized config with Mumford-Shah loss\n",
    "import json\n",
    "\n",
    "config = {\n",
    "    \"description\": \"RGB latent with Mumford-Shah loss for piecewise constant regions\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"output_dir\": \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_mumford_shah\",\n",
    "\n",
    "    \"model\": {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"hidden_size\": 384,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_heads\": 8,\n",
    "        \"ffn_size\": 1536,\n",
    "        \"dropout\": 0.1,\n",
    "        \"grid_size\": 64,\n",
    "        \"num_channels\": 3,  # RGB (3 channels)\n",
    "        \"use_rope\": True,\n",
    "        \"use_smooth_head\": False,\n",
    "        \"tokenizer_name\": \"gpt2\"\n",
    "    },\n",
    "\n",
    "    \"loss\": {\n",
    "        # Loss weights\n",
    "        \"lambda_recon\": 1.0,                  # Reconstruction loss weight\n",
    "        \"lambda_magnitude\": 1.0,              # Magnitude loss weight (prevent collapse)\n",
    "        \"lambda_mumford_shah\": 1.0,           # Mumford-Shah loss (piecewise constant regions)\n",
    "        \n",
    "        # Magnitude parameters\n",
    "        \"min_magnitude\": 0.3,                 # Minimum magnitude target\n",
    "        \n",
    "        # Mumford-Shah parameters\n",
    "        \"mumford_shah_alpha\": 0.0,            # Within-region smoothness (L2 term)\n",
    "        \"mumford_shah_beta\": 5.0              # Boundary sparsity (L1 term) - 0 for non-isotropic\n",
    "    },\n",
    "\n",
    "    \"decoder\": {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"max_seq_len\": 32,\n",
    "        \"hidden_size\": 384,\n",
    "        \"num_layers\": 4,\n",
    "        \"num_heads\": 8,\n",
    "        \"ffn_size\": 1536,\n",
    "        \"dropout\": 0.1,\n",
    "        \"use_rope\": True\n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"batch_size\": 64,  # Reduced for T4 GPU\n",
    "        \"lr\": 2e-4,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.95,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"num_epochs\": 1000,  # High limit - will stop at max_steps\n",
    "        \"max_steps\": 10000,  # Shorter initial training\n",
    "        \"ema_decay\": 0.999,\n",
    "        \"grad_clip\": 1.0,\n",
    "        \"blur_sigma\": 0.8,\n",
    "        \"blur_warmup_steps\": 0,\n",
    "        \"jitter_std\": 0.001  # Latent jittering to prevent speckles\n",
    "    },\n",
    "\n",
    "    \"data\": {\n",
    "        \"train_file\": \"../train_sentences.txt\",\n",
    "        \"val_file\": \"../val_sentences.txt\",\n",
    "        \"num_workers\": 2,  # Colab-optimized\n",
    "        \"file_format\": \"txt\"\n",
    "    },\n",
    "\n",
    "    \"eval\": {\n",
    "        \"eval_interval\": 500,\n",
    "        \"save_interval\": 2000,\n",
    "        \"num_fixed_sentences\": 16\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save config\n",
    "!mkdir -p phase1/configs\n",
    "with open('phase1/configs/phase1_colab.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration created:\")\n",
    "print(f\"  Device: {config['device']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Max steps: {config['training']['max_steps']}\")\n",
    "print(f\"  Channels: {config['model']['num_channels']} (RGB)\")\n",
    "print(f\"\\nLoss components:\")\n",
    "print(f\"  - Reconstruction: {config['loss']['lambda_recon']}\")\n",
    "print(f\"  - Magnitude: {config['loss']['lambda_magnitude']}\")\n",
    "print(f\"  - Mumford-Shah: {config['loss']['lambda_mumford_shah']}\")\n",
    "print(f\"\\nMumford-Shah settings:\")\n",
    "print(f\"  - Alpha (smoothness): {config['loss']['mumford_shah_alpha']}\")\n",
    "print(f\"  - Beta (boundary): {config['loss']['mumford_shah_beta']}\")\n",
    "print(f\"\\nJittering:\")\n",
    "print(f\"  - Jitter std: {config['training']['jitter_std']}\")\n",
    "print(f\"\\nOutput: {config['output_dir']}\")\n",
    "print(\"\\n✓ Ready to train with Mumford-Shah loss for smooth regions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 4. Training\n",
    "\n",
    "This will take approximately **2-3 hours** on a T4 GPU.\n",
    "\n",
    "The training loop will:\n",
    "- Train for up to 50,000 steps (or 10 epochs)\n",
    "- Evaluate every 500 steps\n",
    "- Save checkpoints every 2,000 steps to Google Drive\n",
    "- Display progress bars and loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oates/Desktop/mydocuments/projects/current/blind_lm/blind_lm/phase1\n",
      "======================================================================\n",
      "Starting Phase 1 Training\n",
      "======================================================================\n",
      "This will take approximately 2-3 hours on T4 GPU\n",
      "You can monitor progress below...\n",
      "======================================================================\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/pathlib.py\", line 1311, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_mumford_shah'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/pathlib.py\", line 1311, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/blind_lm_outputs'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/pathlib.py\", line 1311, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/pathlib.py\", line 1311, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/oates/Desktop/mydocuments/projects/current/blind_lm/blind_lm/phase1/train.py\", line 782, in <module>\n",
      "    main()\n",
      "  File \"/Users/oates/Desktop/mydocuments/projects/current/blind_lm/blind_lm/phase1/train.py\", line 777, in main\n",
      "    trainer = Trainer(config)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/oates/Desktop/mydocuments/projects/current/blind_lm/blind_lm/phase1/train.py\", line 155, in __init__\n",
      "    self.output_dir.mkdir(parents=True, exist_ok=True)\n",
      "  File \"/opt/anaconda3/lib/python3.12/pathlib.py\", line 1315, in mkdir\n",
      "    self.parent.mkdir(parents=True, exist_ok=True)\n",
      "  File \"/opt/anaconda3/lib/python3.12/pathlib.py\", line 1315, in mkdir\n",
      "    self.parent.mkdir(parents=True, exist_ok=True)\n",
      "  File \"/opt/anaconda3/lib/python3.12/pathlib.py\", line 1315, in mkdir\n",
      "    self.parent.mkdir(parents=True, exist_ok=True)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/opt/anaconda3/lib/python3.12/pathlib.py\", line 1311, in mkdir\n",
      "    os.mkdir(self, mode)\n",
      "OSError: [Errno 30] Read-only file system: '/content'\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "%cd phase1\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Starting Phase 1 Training\")\n",
    "print(\"=\"*70)\n",
    "print(\"This will take approximately 2-3 hours on T4 GPU\")\n",
    "print(\"You can monitor progress below...\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "!python train.py --config configs/phase1_colab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "generate_report"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample reconstructions...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load config\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     21\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Load models\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/config.json'"
     ]
    }
   ],
   "source": [
    "# Simple evaluation - just show some sample reconstructions from training data\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add phase1 directory to path\n",
    "sys.path.insert(0, '/content/blind_lm/phase1')\n",
    "\n",
    "from model import create_model\n",
    "from decoder_nonar import create_decoder\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "\n",
    "print(\"Generating sample reconstructions...\")\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce'\n",
    "\n",
    "# Load config\n",
    "with open(os.path.join(output_dir, 'config.json')) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "encoder = create_model(config['model']).to(device)\n",
    "decoder = create_decoder(config['decoder']).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = os.path.join(output_dir, 'checkpoint_latest.pt')\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(f'Loaded checkpoint from step {checkpoint[\"step\"]}')\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load test sentences from training data (use sentences the model has seen)\n",
    "train_file = '/content/blind_lm/train_sentences.txt'\n",
    "with open(train_file, 'r') as f:\n",
    "    all_sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Sample 5 random sentences from training data\n",
    "random.seed(42)\n",
    "test_sentences = random.sample(all_sentences, min(5, len(all_sentences)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE RECONSTRUCTIONS (from training data)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentence in test_sentences:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding='max_length', \n",
    "                         truncation=True, max_length=64)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        # Encode\n",
    "        latent = encoder(input_ids, attention_mask)\n",
    "        \n",
    "        # Decode (non-autoregressive - ignores input_ids)\n",
    "        logits = decoder(latent, input_ids, attention_mask)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Decode text\n",
    "        reconstruction = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nOriginal:       {sentence}\")\n",
    "        print(f\"Reconstruction: {reconstruction}\")\n",
    "        \n",
    "        # Check exact match\n",
    "        if sentence.strip() == reconstruction.strip():\n",
    "            print(\"✓ EXACT MATCH\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_results"
   },
   "outputs": [],
   "source": [
    "# Display evaluation results for RGB latents (using training data)\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Add phase1 directory to path\n",
    "sys.path.insert(0, '/content/blind_lm/phase1')\n",
    "\n",
    "eval_dir = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/eval_report\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RGB LATENT EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if we can visualize the RGB latents directly\n",
    "checkpoint_path = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/checkpoint_latest.pt\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Load the checkpoint to get some sample latents\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"\\nCheckpoint Info:\")\n",
    "    print(f\"  Step: {checkpoint['step']}\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    \n",
    "    # Display loss components if available\n",
    "    if 'metrics_history' in checkpoint and checkpoint['metrics_history']:\n",
    "        latest = checkpoint['metrics_history'][-1]\n",
    "        print(f\"\\nLatest Loss Components:\")\n",
    "        if 'loss_components' in latest:\n",
    "            for name, value in latest['loss_components'].items():\n",
    "                print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "# Try to display RGB visualizations if they exist\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RGB VISUALIZATIONS (from training data)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Helper function to convert latent to RGB for display\n",
    "def latent_to_rgb(latent_tensor):\n",
    "    \"\"\"Convert [H, W, 3] tensor to displayable RGB, normalized to [0, 1]\"\"\"\n",
    "    rgb = latent_tensor.cpu().numpy()\n",
    "    # Normalize from [-1.5, 1.5] to [0, 1]\n",
    "    rgb = (rgb + 1.5) / 3.0\n",
    "    rgb = np.clip(rgb, 0, 1)\n",
    "    return rgb\n",
    "\n",
    "# Generate a simple visualization of the RGB latents\n",
    "try:\n",
    "    from model import create_model\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load config\n",
    "    config_path = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce/config.json\"\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        encoder = create_model(config['model']).to(device)\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        encoder.eval()\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load test sentences from training data\n",
    "        train_file = '/content/blind_lm/train_sentences.txt'\n",
    "        with open(train_file, 'r') as f:\n",
    "            all_sentences = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        # Sample 4 random sentences from training data\n",
    "        random.seed(42)\n",
    "        test_sentences = random.sample(all_sentences, min(4, len(all_sentences)))\n",
    "        \n",
    "        # Create figure for RGB images\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, sentence in enumerate(test_sentences):\n",
    "                # Tokenize\n",
    "                inputs = tokenizer(sentence, return_tensors='pt', padding='max_length', \n",
    "                                 truncation=True, max_length=64)\n",
    "                input_ids = inputs['input_ids'].to(device)\n",
    "                attention_mask = inputs['attention_mask'].to(device)\n",
    "                \n",
    "                # Generate latent\n",
    "                latent = encoder(input_ids, attention_mask)  # [1, 32, 32, 3]\n",
    "                \n",
    "                # Convert to RGB for display\n",
    "                rgb = latent_to_rgb(latent[0])\n",
    "                \n",
    "                # Display\n",
    "                axes[i].imshow(rgb)\n",
    "                axes[i].set_title(f'\"{sentence[:30]}...\"' if len(sentence) > 30 else f'\"{sentence}\"', \n",
    "                                 fontsize=8)\n",
    "                axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('RGB Latents (Training Data)', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/content/rgb_samples.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✓ RGB visualization generated!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not generate live visualization: {e}\")\n",
    "    print(\"This is normal if training hasn't completed yet.\")\n",
    "\n",
    "# Show any saved visualizations\n",
    "if os.path.exists(eval_dir):\n",
    "    viz_files = os.listdir(eval_dir)\n",
    "    if viz_files:\n",
    "        print(f\"\\nSaved visualizations in {eval_dir}:\")\n",
    "        for file in viz_files:\n",
    "            if file.endswith('.png'):\n",
    "                print(f\"  - {file}\")\n",
    "                display(Image(os.path.join(eval_dir, file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interpret"
   },
   "source": [
    "## 7. Interpret Results\n",
    "\n",
    "### What to Expect with RGB InfoNCE\n",
    "\n",
    "With **3-channel RGB** output and **InfoNCE patch coherence**, you should see:\n",
    "\n",
    "**RGB Color Images**:\n",
    "- Full-color visualizations showing all 3 channels (R, G, B)\n",
    "- Spatially coherent colored patterns (nearby regions have similar colors)\n",
    "- Local smoothness with global diversity (different parts of image can have different colors)\n",
    "- Different sentences produce distinct colored patterns\n",
    "\n",
    "**InfoNCE Coherence Effects**:\n",
    "- Nearby patches (within positive_radius ~3 pixels) will be similar in color\n",
    "- Distant patches (beyond negative_radius ~11 pixels) will differ\n",
    "- Creates smooth, blob-like colored regions\n",
    "- Prevents noisy, scattered pixels\n",
    "\n",
    "**Reconstruction Quality**:\n",
    "- High reconstruction accuracy (target >40% exact match)\n",
    "- Semantically similar sentences produce similar RGB patterns\n",
    "- Different sentences produce distinct RGB patterns\n",
    "- Text decoder can reconstruct original text from RGB latent\n",
    "\n",
    "### Loss Components to Monitor\n",
    "\n",
    "- **Reconstruction loss**: Should decrease steadily (target < 1.0)\n",
    "  - Measures how well the text decoder reconstructs the input\n",
    "  \n",
    "- **InfoNCE loss**: Should stabilize after initial decrease\n",
    "  - Measures spatial coherence of RGB patches\n",
    "  - Lower = more coherent colored regions\n",
    "  \n",
    "- **Magnitude loss**: Should approach zero as training progresses\n",
    "  - Ensures latent doesn't collapse to all zeros\n",
    "  - Maintains meaningful signal strength\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "- **If patterns are too noisy**: Increase `lambda_infonce` or decrease `infonce_temperature`\n",
    "- **If patterns are too uniform**: Decrease `lambda_infonce` or increase `infonce_positive_radius`\n",
    "- **If reconstruction is poor**: Increase `lambda_recon`\n",
    "- **If latents collapse**: Increase `lambda_magnitude`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 8. Download Checkpoints (Optional)\n",
    "\n",
    "Download the final checkpoint and visualizations to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Create a zip file with important results\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "output_dir = \"/content/drive/MyDrive/blind_lm_outputs/phase1_rgb_infonce\"\n",
    "zip_path = \"/content/phase1_rgb_infonce_results.zip\"\n",
    "\n",
    "print(\"Creating results archive...\")\n",
    "\n",
    "# Create temporary directory\n",
    "temp_dir = \"/content/phase1_results_temp\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Copy important files\n",
    "files_to_include = [\n",
    "    \"config.json\",\n",
    "    \"checkpoint_latest.pt\"\n",
    "]\n",
    "\n",
    "for file in files_to_include:\n",
    "    src = os.path.join(output_dir, file)\n",
    "    if os.path.exists(src):\n",
    "        dst_dir = os.path.join(temp_dir, os.path.dirname(file))\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        shutil.copy2(src, os.path.join(temp_dir, file))\n",
    "        print(f\"  ✓ {file}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {file} not found\")\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive('/content/phase1_rgb_infonce_results', 'zip', temp_dir)\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "print(\"\\nDownloading...\")\n",
    "files.download('/content/phase1_rgb_infonce_results.zip')\n",
    "\n",
    "print(\"\\n✓ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "After Phase 1 passes:\n",
    "\n",
    "1. **Phase 2**: Add semantic meaning via contrastive learning\n",
    "   - Paraphrases should produce similar latents\n",
    "   - Counterfactuals should produce different latents\n",
    "\n",
    "2. **Phase 3**: Spatial jitter robustness\n",
    "   - Latents should be invariant to small shifts\n",
    "\n",
    "3. **Phase 4**: Add text decoder\n",
    "   - Reconstruct text from latent\n",
    "\n",
    "4. **Phase 5**: Round-trip generation\n",
    "   - Generate paraphrases without copying\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the [project documentation](https://github.com/jtooates/blind_lm)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "name": "Phase 1: Image-like Latent Training",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
